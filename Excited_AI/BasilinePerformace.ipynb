{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'stable_baselines'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-de8b30882cc3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mstable_baselines\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeepq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstable_baselines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtf_util\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOffPolicyRLModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSetVerbosity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensorboardWriter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstable_baselines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvec_env\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVecEnv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstable_baselines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschedules\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLinearSchedule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstable_baselines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay_buffer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mReplayBuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPrioritizedReplayBuffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'stable_baselines'"
     ]
    }
   ],
   "source": [
    "from stable_baselines import logger, deepq\n",
    "from stable_baselines.common import tf_util, OffPolicyRLModel, SetVerbosity, TensorboardWriter\n",
    "from stable_baselines.common.vec_env import VecEnv\n",
    "from stable_baselines.common.schedules import LinearSchedule\n",
    "from stable_baselines.deepq.replay_buffer import ReplayBuffer, PrioritizedReplayBuffer\n",
    "from stable_baselines.deepq.policies import DQNPolicy\n",
    "from stable_baselines.a2c.utils import find_trainable_variables, total_episode_reward_logger\n",
    "from functools import partial\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym\n",
    "from stable_baselines.common.policies import MlpPolicy, CnnPolicy\n",
    "from stable_baselines.common.vec_env import DummyVecEnv\n",
    "from stable_baselines.bench import Monitor\n",
    "from stable_baselines.results_plotter import load_results, ts2xy\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines.deepq.policies import FeedForwardPolicy, CnnPolicy\n",
    "from stable_baselines.common.cmd_util import make_atari_env, make_atari\n",
    "from stable_baselines.common.vec_env import VecFrameStack\n",
    "from stable_baselines.common.atari_wrappers import make_atari, MaxAndSkipEnv, NoopResetEnv\n",
    "from stable_baselines.deepq import DQN, wrap_atari_dqn, DQN2\n",
    "from stable_baselines.bench import Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/phaniram/anaconda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import zipfile\n",
    "import cloudpickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    def __init__(self, size):\n",
    "        \"\"\"Create Replay buffer.\n",
    "        Parameters\n",
    "        ----------\n",
    "        size: int\n",
    "            Max number of transitions to store in the buffer. When the buffer\n",
    "            overflows the old memories are dropped.\n",
    "        \"\"\"\n",
    "        self._storage = []\n",
    "        self._maxsize = size\n",
    "        self._next_idx = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._storage)\n",
    "\n",
    "    def add(self, obs_t, action, reward, obs_tp1, done):\n",
    "        data = (obs_t, action, reward, obs_tp1, done)\n",
    "\n",
    "        if self._next_idx >= len(self._storage):\n",
    "            self._storage.append(data)\n",
    "        else:\n",
    "            self._storage[self._next_idx] = data\n",
    "        self._next_idx = (self._next_idx + 1) % self._maxsize\n",
    "\n",
    "    def _encode_sample(self, idxes):\n",
    "        obses_t, actions, rewards, obses_tp1, dones = [], [], [], [], []\n",
    "        for i in idxes:\n",
    "            data = self._storage[i]\n",
    "            obs_t, action, reward, obs_tp1, done = data\n",
    "            obses_t.append(np.array(obs_t, copy=False))\n",
    "            actions.append(np.array(action, copy=False))\n",
    "            rewards.append(reward)\n",
    "            obses_tp1.append(np.array(obs_tp1, copy=False))\n",
    "            dones.append(done)\n",
    "        return np.array(obses_t), np.array(actions), np.array(rewards), np.array(obses_tp1), np.array(dones)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Sample a batch of experiences.\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_size: int\n",
    "            How many transitions to sample.\n",
    "        Returns\n",
    "        -------\n",
    "        obs_batch: np.array\n",
    "            batch of observations\n",
    "        act_batch: np.array\n",
    "            batch of actions executed given obs_batch\n",
    "        rew_batch: np.array\n",
    "            rewards received as results of executing act_batch\n",
    "        next_obs_batch: np.array\n",
    "            next set of observations seen after executing act_batch\n",
    "        done_mask: np.array\n",
    "            done_mask[i] = 1 if executing act_batch[i] resulted in\n",
    "            the end of an episode and 0 otherwise.\n",
    "        \"\"\"\n",
    "        idxes = [random.randint(0, len(self._storage) - 1) for _ in range(batch_size)]\n",
    "        return self._encode_sample(idxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActWrapper(object):\n",
    "    def __init__(self, act, act_params):\n",
    "        self._act = act\n",
    "        self._act_params = act_params\n",
    "        self.initial_state = None\n",
    "\n",
    "    @staticmethod\n",
    "    def load_act(path):\n",
    "        with open(path, \"rb\") as f:\n",
    "            model_data, act_params = cloudpickle.load(f)\n",
    "        act = deepq.build_act(**act_params)\n",
    "        sess = tf.Session()\n",
    "        sess.__enter__()\n",
    "        with tempfile.TemporaryDirectory() as td:\n",
    "            arc_path = os.path.join(td, \"packed.zip\")\n",
    "            with open(arc_path, \"wb\") as f:\n",
    "                f.write(model_data)\n",
    "\n",
    "            zipfile.ZipFile(arc_path, 'r', zipfile.ZIP_DEFLATED).extractall(td)\n",
    "            load_variables(os.path.join(td, \"model\"))\n",
    "\n",
    "        return ActWrapper(act, act_params)\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self._act(*args, **kwargs)\n",
    "\n",
    "    def step(self, observation, **kwargs):\n",
    "        # DQN doesn't use RNNs so we ignore states and masks\n",
    "        kwargs.pop('S', None)\n",
    "        kwargs.pop('M', None)\n",
    "        return self._act([observation], **kwargs), None, None, None\n",
    "\n",
    "    def save_act(self, path=None):\n",
    "        \"\"\"Save model to a pickle located at `path`\"\"\"\n",
    "        if path is None:\n",
    "            cwd = os.getcwd()\n",
    "            path = os.path.join(cwd, \"model.pkl\")\n",
    "\n",
    "        with tempfile.TemporaryDirectory() as td:\n",
    "            save_variables(os.path.join(td, \"model\"))\n",
    "            arc_name = os.path.join(td, \"packed.zip\")\n",
    "            with zipfile.ZipFile(arc_name, 'w') as zipf:\n",
    "                for root, dirs, files in os.walk(td):\n",
    "                    for fname in files:\n",
    "                        file_path = os.path.join(root, fname)\n",
    "                        if file_path != arc_name:\n",
    "                            zipf.write(file_path, os.path.relpath(file_path, td))\n",
    "            with open(arc_name, \"rb\") as f:\n",
    "                model_data = f.read()\n",
    "        with open(path, \"wb\") as f:\n",
    "            cloudpickle.dump((model_data, self._act_params), f)\n",
    "\n",
    "    def save(self, path):\n",
    "        save_variables(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_act(path):\n",
    "    \"\"\"Load act function that was returned by learn function.\n",
    "    Parameters\n",
    "    ----------\n",
    "    path: str\n",
    "        path to the act function pickle\n",
    "    Returns\n",
    "    -------\n",
    "    act: ActWrapper\n",
    "        function that takes a batch of observations\n",
    "        and returns actions.\n",
    "    \"\"\"\n",
    "    return ActWrapper.load_act(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_session(config=None):\n",
    "    \"\"\"Get default session or create one with a given config\"\"\"\n",
    "    sess = tf.get_default_session()\n",
    "    if sess is None:\n",
    "        sess = make_session(config=config, make_default=True)\n",
    "    return sess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/phaniram/Downloads/Tensorflow-Bootcamp-master/Excited_AI/model.pkl'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(cwd, \"model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TemporaryDirectory '/var/folders/p3/w4k0dsmj2332qwlvnhgkl30h0000gn/T/tmplrqdujzb'>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tempfile.TemporaryDirectory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Theano style Function\n",
    "# ================================================================\n",
    "class _Function(object):\n",
    "    def __init__(self, inputs, outputs, updates, givens):\n",
    "        \"\"\"\n",
    "        Example:\n",
    "        x = tf.placeholder(tf.int32, (), name=\"x\")\n",
    "        y = tf.placeholder(tf.int32, (), name=\"y\")\n",
    "        z = 3 * x + 2 * y\n",
    "        lin = function([x, y], z, givens={y: 0})\n",
    "        with single_threaded_session():\n",
    "            initialize()\n",
    "            assert lin(2) == 6\n",
    "            assert lin(x=3) == 9\n",
    "            assert lin(2, 2) == 10\n",
    "            assert lin(x=2, y=3) == 12\n",
    "\n",
    "        \"\"\"\n",
    "        for inpt in inputs:\n",
    "            if not hasattr(inpt, 'make_feed_dict') and not (type(inpt) is tf.Tensor and len(inpt.op.inputs) == 0):\n",
    "                assert False, \"inputs should all be placeholders, constants, or have a make_feed_dict method\"\n",
    "        self.inputs = inputs\n",
    "        self.input_names = {inp.name.split(\"/\")[-1].split(\":\")[0]: inp for inp in inputs}\n",
    "        updates = updates or []\n",
    "        self.update_group = tf.group(*updates)\n",
    "        self.outputs_update = list(outputs) + [self.update_group]\n",
    "        self.givens = {} if givens is None else givens\n",
    "\n",
    "    def _feed_input(self, feed_dict, inpt, value):\n",
    "        if hasattr(inpt, 'make_feed_dict'):\n",
    "            feed_dict.update(inpt.make_feed_dict(value))\n",
    "        else:\n",
    "            feed_dict[inpt] = adjust_shape(inpt, value)\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        assert len(args) + len(kwargs) <= len(self.inputs), \"Too many arguments provided\"\n",
    "        feed_dict = {}\n",
    "        # Update feed dict with givens.\n",
    "        for inpt in self.givens:\n",
    "            feed_dict[inpt] = adjust_shape(inpt, feed_dict.get(inpt, self.givens[inpt]))\n",
    "        # Update the args\n",
    "        for inpt, value in zip(self.inputs, args):\n",
    "            self._feed_input(feed_dict, inpt, value)\n",
    "        for inpt_name, value in kwargs.items():\n",
    "            self._feed_input(feed_dict, self.input_names[inpt_name], value)\n",
    "        results = get_session().run(self.outputs_update, feed_dict=feed_dict)[:-1]\n",
    "        return results\n",
    "    \n",
    "    \n",
    "def function(inputs, outputs, updates=None, givens=None):\n",
    "    if isinstance(outputs, list):\n",
    "        return _Function(inputs, outputs, updates, givens=givens)\n",
    "    elif isinstance(outputs, (dict, collections.OrderedDict)):\n",
    "        f = _Function(inputs, outputs.values(), updates, givens=givens)\n",
    "        return lambda *args, **kwargs: type(outputs)(zip(outputs.keys(), f(*args, **kwargs)))\n",
    "    else:\n",
    "        f = _Function(inputs, [outputs], updates, givens=givens)\n",
    "        return lambda *args, **kwargs: f(*args, **kwargs)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _cnn_to_mlp(convs, hiddens, dueling, input_, num_actions, scope, reuse=False, layer_norm=False):\n",
    "    \n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        out = input_\n",
    "        with tf.variable_scope(\"convnet\"):\n",
    "            for num_outputs, kernel_size, stride in convs:\n",
    "                out = layers.convolution2d(out,\n",
    "                                           num_outputs=num_outputs,\n",
    "                                           kernel_size=kernel_size,\n",
    "                                           stride=stride,\n",
    "                                           activation_fn=tf.nn.relu)\n",
    "        conv_out = layers.flatten(out)\n",
    "        with tf.variable_scope(\"action_value\"):\n",
    "            action_out = conv_out\n",
    "            for hidden in hiddens:\n",
    "                action_out = layers.fully_connected(action_out, num_outputs=hidden, activation_fn=None)\n",
    "                if layer_norm:\n",
    "                    action_out = layers.layer_norm(action_out, center=True, scale=True)\n",
    "                action_out = tf.nn.relu(action_out)\n",
    "            action_scores = layers.fully_connected(action_out, num_outputs=num_actions, activation_fn=None)\n",
    "\n",
    "        if dueling:\n",
    "            with tf.variable_scope(\"state_value\"):\n",
    "                state_out = conv_out\n",
    "                for hidden in hiddens:\n",
    "                    state_out = layers.fully_connected(state_out, num_outputs=hidden, activation_fn=None)\n",
    "                    if layer_norm:\n",
    "                        state_out = layers.layer_norm(state_out, center=True, scale=True)\n",
    "                    state_out = tf.nn.relu(state_out)\n",
    "                state_score = layers.fully_connected(state_out, num_outputs=1, activation_fn=None)\n",
    "            action_scores_mean = tf.reduce_mean(action_scores, 1)\n",
    "            action_scores_centered = action_scores - tf.expand_dims(action_scores_mean, 1)\n",
    "            q_out = state_score + action_scores_centered\n",
    "        else:\n",
    "            q_out = action_scores\n",
    "        return q_out\n",
    "\n",
    "    \n",
    "#dummy function to pass in arguments\n",
    "def cnn_to_mlp(convs, hiddens, dueling=False, layer_norm=False):return lambda *args, **kwargs: _cnn_to_mlp(convs, hiddens, dueling, layer_norm=layer_norm, *args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_q_func(network, hiddens=[256], dueling=False, layer_norm=False, **network_kwargs):\n",
    "    if isinstance(network, str):\n",
    "        from baselines.common.models import get_network_builder\n",
    "        network = get_network_builder(network)(**network_kwargs)\n",
    "\n",
    "    def q_func_builder(input_placeholder, num_actions, scope, reuse=False):\n",
    "        with tf.variable_scope(scope, reuse=reuse):\n",
    "            latent = network(input_placeholder)\n",
    "            if isinstance(latent, tuple):\n",
    "                if latent[1] is not None:\n",
    "                    raise NotImplementedError(\"DQN is not compatible with recurrent policies yet\")\n",
    "                latent = latent[0]\n",
    "\n",
    "            latent = layers.flatten(latent)\n",
    "\n",
    "            with tf.variable_scope(\"action_value\"):\n",
    "                action_out = latent\n",
    "                for hidden in hiddens:\n",
    "                    action_out = layers.fully_connected(action_out, num_outputs=hidden, activation_fn=None)\n",
    "                    if layer_norm:\n",
    "                        action_out = layers.layer_norm(action_out, center=True, scale=True)\n",
    "                    action_out = tf.nn.relu(action_out)\n",
    "                action_scores = layers.fully_connected(action_out, num_outputs=num_actions, activation_fn=None)\n",
    "\n",
    "            if dueling:\n",
    "                with tf.variable_scope(\"state_value\"):\n",
    "                    state_out = latent\n",
    "                    for hidden in hiddens:\n",
    "                        state_out = layers.fully_connected(state_out, num_outputs=hidden, activation_fn=None)\n",
    "                        if layer_norm:\n",
    "                            state_out = layers.layer_norm(state_out, center=True, scale=True)\n",
    "                        state_out = tf.nn.relu(state_out)\n",
    "                    state_score = layers.fully_connected(state_out, num_outputs=1, activation_fn=None)\n",
    "                action_scores_mean = tf.reduce_mean(action_scores, 1)\n",
    "                action_scores_centered = action_scores - tf.expand_dims(action_scores_mean, 1)\n",
    "                q_out = state_score + action_scores_centered\n",
    "            else:\n",
    "                q_out = action_scores\n",
    "            return q_out\n",
    "\n",
    "    return q_func_builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_train(make_obs_ph, q_func, num_actions, optimizer, grad_norm_clipping=None, gamma=1.0,\n",
    "    double_q=False, scope=\"deepq\", reuse=None, param_noise=False, param_noise_filter_func=None):\n",
    "    if param_noise:\n",
    "        act_f = build_act_with_param_noise(make_obs_ph, q_func, num_actions, scope=scope, reuse=reuse,\n",
    "            param_noise_filter_func=param_noise_filter_func)\n",
    "    else:\n",
    "        act_f = build_act(make_obs_ph, q_func, num_actions, scope=scope, reuse=reuse)\n",
    "\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        # set up placeholders\n",
    "        obs_t_input = make_obs_ph(\"obs_t\")\n",
    "        act_t_ph = tf.placeholder(tf.int32, [None], name=\"action\")\n",
    "        rew_t_ph = tf.placeholder(tf.float32, [None], name=\"reward\")\n",
    "        obs_tp1_input = make_obs_ph(\"obs_tp1\")\n",
    "        done_mask_ph = tf.placeholder(tf.float32, [None], name=\"done\")\n",
    "        importance_weights_ph = tf.placeholder(tf.float32, [None], name=\"weight\")\n",
    "\n",
    "        # q network evaluation\n",
    "        q_t = q_func(obs_t_input.get(), num_actions, scope=\"q_func\", reuse=True)  # reuse parameters from act\n",
    "        q_func_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=tf.get_variable_scope().name + \"/q_func\")\n",
    "\n",
    "        # target q network evalution\n",
    "        q_tp1 = q_func(obs_tp1_input.get(), num_actions, scope=\"target_q_func\")\n",
    "        target_q_func_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=tf.get_variable_scope().name + \"/target_q_func\")\n",
    "\n",
    "        # q scores for actions which we know were selected in the given state.\n",
    "        q_t_selected = tf.reduce_sum(q_t * tf.one_hot(act_t_ph, num_actions), 1)\n",
    "\n",
    "        # compute estimate of best possible value starting from state at t + 1\n",
    "        if double_q:\n",
    "            q_tp1_using_online_net = q_func(obs_tp1_input.get(), num_actions, scope=\"q_func\", reuse=True)\n",
    "            q_tp1_best_using_online_net = tf.argmax(q_tp1_using_online_net, 1)\n",
    "            q_tp1_best = tf.reduce_sum(q_tp1 * tf.one_hot(q_tp1_best_using_online_net, num_actions), 1)\n",
    "        else:\n",
    "            q_tp1_best = tf.reduce_max(q_tp1, 1)\n",
    "        q_tp1_best_masked = (1.0 - done_mask_ph) * q_tp1_best\n",
    "\n",
    "        # compute RHS of bellman equation\n",
    "        q_t_selected_target = rew_t_ph + gamma * q_tp1_best_masked\n",
    "\n",
    "        # compute the error (potentially clipped)\n",
    "        td_error = q_t_selected - tf.stop_gradient(q_t_selected_target)\n",
    "        errors = U.huber_loss(td_error)\n",
    "        weighted_error = tf.reduce_mean(importance_weights_ph * errors)\n",
    "\n",
    "        # compute optimization op (potentially with gradient clipping)\n",
    "        if grad_norm_clipping is not None:\n",
    "            gradients = optimizer.compute_gradients(weighted_error, var_list=q_func_vars)\n",
    "            for i, (grad, var) in enumerate(gradients):\n",
    "                if grad is not None:\n",
    "                    gradients[i] = (tf.clip_by_norm(grad, grad_norm_clipping), var)\n",
    "            optimize_expr = optimizer.apply_gradients(gradients)\n",
    "        else:\n",
    "            optimize_expr = optimizer.minimize(weighted_error, var_list=q_func_vars)\n",
    "\n",
    "        # update_target_fn will be called periodically to copy Q network to target Q network\n",
    "        update_target_expr = []\n",
    "        for var, var_target in zip(sorted(q_func_vars, key=lambda v: v.name),\n",
    "                                   sorted(target_q_func_vars, key=lambda v: v.name)):\n",
    "            update_target_expr.append(var_target.assign(var))\n",
    "        update_target_expr = tf.group(*update_target_expr)\n",
    "\n",
    "        # Create callable functions\n",
    "        train = U.function(\n",
    "            inputs=[\n",
    "                obs_t_input,\n",
    "                act_t_ph,\n",
    "                rew_t_ph,\n",
    "                obs_tp1_input,\n",
    "                done_mask_ph,\n",
    "                importance_weights_ph\n",
    "            ],\n",
    "            outputs=td_error,\n",
    "            updates=[optimize_expr]\n",
    "        )\n",
    "        update_target = U.function([], [], updates=[update_target_expr])\n",
    "\n",
    "        q_values = U.function([obs_t_input], q_t)\n",
    "\n",
    "        return act_f, train, update_target, {'q_values': q_values}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for root, dirs, files in os.walk(td)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(env,\n",
    "          network,\n",
    "          seed=None,\n",
    "          lr=5e-4,\n",
    "          total_timesteps=100000,\n",
    "          buffer_size=50000,\n",
    "          exploration_fraction=0.1,\n",
    "          exploration_final_eps=0.02,\n",
    "          train_freq=1,\n",
    "          batch_size=32,\n",
    "          print_freq=100,\n",
    "          checkpoint_freq=10000,\n",
    "          checkpoint_path=None,\n",
    "          learning_starts=1000,\n",
    "          gamma=1.0,\n",
    "          target_network_update_freq=500,\n",
    "          prioritized_replay=False,\n",
    "          param_noise=False,\n",
    "          callback=None,\n",
    "          load_path=None,\n",
    "          **network_kwargs\n",
    "            ):\n",
    "    \n",
    "    sess = get_session()\n",
    "    set_global_seeds(seed)\n",
    "    q_func = build_q_func(network, **network_kwargs)\n",
    "    observation_space = env.observation_space\n",
    "    \n",
    "    \n",
    "    act, train, update_target, debug = deepq.build_train(\n",
    "        make_obs_ph=make_obs_ph,\n",
    "        q_func=q_func,\n",
    "        num_actions=env.action_space.n,\n",
    "        optimizer=tf.train.AdamOptimizer(learning_rate=lr),\n",
    "        gamma=gamma,\n",
    "        grad_norm_clipping=10,\n",
    "        param_noise=param_noise\n",
    "    )\n",
    "    \n",
    "    act_params = {\n",
    "        'make_obs_ph': make_obs_ph,\n",
    "        'q_func': q_func,\n",
    "        'num_actions': env.action_space.n,\n",
    "    }\n",
    "    \n",
    "    act = ActWrapper(act, act_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
