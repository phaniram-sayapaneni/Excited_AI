{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/phaniram/anaconda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/phaniram/anaconda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines import logger, deepq\n",
    "from stable_baselines.common import tf_util, OffPolicyRLModel, SetVerbosity, TensorboardWriter\n",
    "from stable_baselines.common.vec_env import VecEnv\n",
    "from stable_baselines.common.schedules import LinearSchedule\n",
    "from stable_baselines.deepq.replay_buffer import ReplayBuffer, PrioritizedReplayBuffer\n",
    "from stable_baselines.deepq.policies import DQNPolicy\n",
    "from stable_baselines.a2c.utils import find_trainable_variables, total_episode_reward_logger\n",
    "from functools import partial\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym\n",
    "from stable_baselines.common.policies import MlpPolicy, CnnPolicy\n",
    "from stable_baselines.common.vec_env import DummyVecEnv\n",
    "from stable_baselines.bench import Monitor\n",
    "from stable_baselines.results_plotter import load_results, ts2xy\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines.deepq.policies import FeedForwardPolicy, CnnPolicy\n",
    "from stable_baselines.common.cmd_util import make_atari_env, make_atari\n",
    "from stable_baselines.common.vec_env import VecFrameStack\n",
    "from stable_baselines.common.atari_wrappers import make_atari, MaxAndSkipEnv, NoopResetEnv\n",
    "from stable_baselines.deepq import DQN, wrap_atari_dqn#, DQN2\n",
    "from stable_baselines.bench import Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(OffPolicyRLModel):\n",
    "    \"\"\"\n",
    "    The DQN model class. DQN paper: https://arxiv.org/pdf/1312.5602.pdf\n",
    "    :param policy: (DQNPolicy or str) The policy model to use (MlpPolicy, CnnPolicy, LnMlpPolicy, ...)\n",
    "    :param env: (Gym environment or str) The environment to learn from (if registered in Gym, can be str)\n",
    "    :param gamma: (float) discount factor\n",
    "    :param learning_rate: (float) learning rate for adam optimizer\n",
    "    :param buffer_size: (int) size of the replay buffer\n",
    "    :param exploration_fraction: (float) fraction of entire training period over which the exploration rate is\n",
    "            annealed\n",
    "    :param exploration_final_eps: (float) final value of random action probability\n",
    "    :param train_freq: (int) update the model every `train_freq` steps. set to None to disable printing\n",
    "    :param batch_size: (int) size of a batched sampled from replay buffer for training\n",
    "    :param checkpoint_freq: (int) how often to save the model. This is so that the best version is restored at the\n",
    "            end of the training. If you do not wish to restore the best version\n",
    "            at the end of the training set this variable to None.\n",
    "    :param checkpoint_path: (str) replacement path used if you need to log to somewhere else than a temporary\n",
    "            directory.\n",
    "    :param learning_starts: (int) how many steps of the model to collect transitions for before learning starts\n",
    "    :param target_network_update_freq: (int) update the target network every `target_network_update_freq` steps.\n",
    "    :param prioritized_replay: (bool) if True prioritized replay buffer will be used.\n",
    "    :param prioritized_replay_alpha: (float)alpha parameter for prioritized replay buffer.\n",
    "        It determines how much prioritization is used, with alpha=0 corresponding to the uniform case.\n",
    "    :param prioritized_replay_beta0: (float) initial value of beta for prioritized replay buffer\n",
    "    :param prioritized_replay_beta_iters: (int) number of iterations over which beta will be annealed from initial\n",
    "            value to 1.0. If set to None equals to max_timesteps.\n",
    "    :param prioritized_replay_eps: (float) epsilon to add to the TD errors when updating priorities.\n",
    "    :param param_noise: (bool) Whether or not to apply noise to the parameters of the policy.\n",
    "    :param verbose: (int) the verbosity level: 0 none, 1 training information, 2 tensorflow debug\n",
    "    :param tensorboard_log: (str) the log location for tensorboard (if None, no logging)\n",
    "    :param _init_setup_model: (bool) Whether or not to build the network at the creation of the instance\n",
    "    :param full_tensorboard_log: (bool) enable additional logging when using tensorboard\n",
    "        WARNING: this logging can take a lot of space quickly\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, policy, env, gamma=0.99, learning_rate=5e-4, buffer_size=50000, exploration_fraction=0.1,\n",
    "                 exploration_final_eps=0.02, train_freq=1, batch_size=32, checkpoint_freq=10000, checkpoint_path=None,\n",
    "                 learning_starts=1000, target_network_update_freq=500, prioritized_replay=False,\n",
    "                 prioritized_replay_alpha=0.6, prioritized_replay_beta0=0.4, prioritized_replay_beta_iters=None,\n",
    "                 prioritized_replay_eps=1e-6, param_noise=False, verbose=0, tensorboard_log=None,\n",
    "                 _init_setup_model=True, policy_kwargs=None, full_tensorboard_log=False):\n",
    "\n",
    "        # TODO: replay_buffer refactoring\n",
    "        super(DQN, self).__init__(policy=policy, env=env, replay_buffer=None, verbose=verbose, policy_base=DQNPolicy,\n",
    "                                  requires_vec_env=False, policy_kwargs=policy_kwargs)\n",
    "\n",
    "        self.checkpoint_path = checkpoint_path\n",
    "        self.param_noise = param_noise\n",
    "        self.learning_starts = learning_starts\n",
    "        self.train_freq = train_freq\n",
    "        self.prioritized_replay = prioritized_replay\n",
    "        self.prioritized_replay_eps = prioritized_replay_eps\n",
    "        self.batch_size = batch_size\n",
    "        self.target_network_update_freq = target_network_update_freq\n",
    "        self.checkpoint_freq = checkpoint_freq\n",
    "        self.prioritized_replay_alpha = prioritized_replay_alpha\n",
    "        self.prioritized_replay_beta0 = prioritized_replay_beta0\n",
    "        self.prioritized_replay_beta_iters = prioritized_replay_beta_iters\n",
    "        self.exploration_final_eps = exploration_final_eps\n",
    "        self.exploration_fraction = exploration_fraction\n",
    "        self.buffer_size = buffer_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.tensorboard_log = tensorboard_log\n",
    "        self.full_tensorboard_log = full_tensorboard_log\n",
    "\n",
    "        self.graph = None\n",
    "        self.sess = None\n",
    "        self._train_step = None\n",
    "        self.step_model = None\n",
    "        self.update_target = None\n",
    "        self.act = None\n",
    "        self.proba_step = None\n",
    "        self.replay_buffer = None\n",
    "        self.beta_schedule = None\n",
    "        self.exploration = None\n",
    "        self.params = None\n",
    "        self.summary = None\n",
    "        self.episode_reward = None\n",
    "\n",
    "        if _init_setup_model:\n",
    "            self.setup_model()\n",
    "\n",
    "    def _get_pretrain_placeholders(self):\n",
    "        policy = self.step_model\n",
    "        return policy.obs_ph, tf.placeholder(tf.int32, [None]), policy.q_values\n",
    "\n",
    "    def setup_model(self):\n",
    "\n",
    "        with SetVerbosity(self.verbose):\n",
    "            assert not isinstance(self.action_space, gym.spaces.Box), \\\n",
    "                \"Error: DQN cannot output a gym.spaces.Box action space.\"\n",
    "\n",
    "            # If the policy is wrap in functool.partial (e.g. to disable dueling)\n",
    "            # unwrap it to check the class type\n",
    "            if isinstance(self.policy, partial):\n",
    "                test_policy = self.policy.func\n",
    "            else:\n",
    "                test_policy = self.policy\n",
    "            assert issubclass(test_policy, DQNPolicy), \"Error: the input policy for the DQN model must be \" \\\n",
    "                                                       \"an instance of DQNPolicy.\"\n",
    "\n",
    "            self.graph = tf.Graph()\n",
    "            with self.graph.as_default():\n",
    "                self.sess = tf_util.make_session(graph=self.graph)\n",
    "\n",
    "                optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
    "\n",
    "                self.act, self._train_step, self.update_target, self.step_model = deepq.build_train(\n",
    "                    q_func=partial(self.policy, **self.policy_kwargs),\n",
    "                    ob_space=self.observation_space,\n",
    "                    ac_space=self.action_space,\n",
    "                    optimizer=optimizer,\n",
    "                    gamma=self.gamma,\n",
    "                    grad_norm_clipping=10,\n",
    "                    param_noise=self.param_noise,\n",
    "                    sess=self.sess,\n",
    "                    full_tensorboard_log=self.full_tensorboard_log\n",
    "                )\n",
    "                self.proba_step = self.step_model.proba_step\n",
    "                self.params = find_trainable_variables(\"deepq\")\n",
    "\n",
    "                # Initialize the parameters and copy them to the target network.\n",
    "                tf_util.initialize(self.sess)\n",
    "                self.update_target(sess=self.sess)\n",
    "\n",
    "                self.summary = tf.summary.merge_all()\n",
    "\n",
    "    def learn(self, total_timesteps, callback=None, seed=None, log_interval=100, tb_log_name=\"DQN\",\n",
    "              reset_num_timesteps=True):\n",
    "\n",
    "        new_tb_log = self._init_num_timesteps(reset_num_timesteps)\n",
    "\n",
    "        with SetVerbosity(self.verbose), TensorboardWriter(self.graph, self.tensorboard_log, tb_log_name, new_tb_log) \\\n",
    "                as writer:\n",
    "            self._setup_learn(seed)\n",
    "\n",
    "            # Create the replay buffer\n",
    "            if self.prioritized_replay:\n",
    "                self.replay_buffer = PrioritizedReplayBuffer(self.buffer_size, alpha=self.prioritized_replay_alpha)\n",
    "                if self.prioritized_replay_beta_iters is None:\n",
    "                    prioritized_replay_beta_iters = total_timesteps\n",
    "                else:\n",
    "                    prioritized_replay_beta_iters = self.prioritized_replay_beta_iters\n",
    "                self.beta_schedule = LinearSchedule(prioritized_replay_beta_iters,\n",
    "                                                    initial_p=self.prioritized_replay_beta0,\n",
    "                                                    final_p=1.0)\n",
    "            else:\n",
    "                self.replay_buffer = ReplayBuffer(self.buffer_size)\n",
    "                self.beta_schedule = None\n",
    "            # Create the schedule for exploration starting from 1.\n",
    "            self.exploration = LinearSchedule(schedule_timesteps=int(self.exploration_fraction * total_timesteps),\n",
    "                                              initial_p=1.0,\n",
    "                                              final_p=self.exploration_final_eps)\n",
    "\n",
    "            episode_rewards = [0.0]\n",
    "            obs = self.env.reset()\n",
    "            reset = True\n",
    "            self.episode_reward = np.zeros((1,))\n",
    "\n",
    "            for _ in range(total_timesteps):\n",
    "                if callback is not None:\n",
    "                    # Only stop training if return value is False, not when it is None. This is for backwards\n",
    "                    # compatibility with callbacks that have no return statement.\n",
    "                    if callback(locals(), globals()) is False:\n",
    "                        break\n",
    "                # Take action and update exploration to the newest value\n",
    "                kwargs = {}\n",
    "                if not self.param_noise:\n",
    "                    update_eps = self.exploration.value(self.num_timesteps)\n",
    "                    update_param_noise_threshold = 0.\n",
    "                else:\n",
    "                    update_eps = 0.\n",
    "                    # Compute the threshold such that the KL divergence between perturbed and non-perturbed\n",
    "                    # policy is comparable to eps-greedy exploration with eps = exploration.value(t).\n",
    "                    # See Appendix C.1 in Parameter Space Noise for Exploration, Plappert et al., 2017\n",
    "                    # for detailed explanation.\n",
    "                    update_param_noise_threshold = \\\n",
    "                        -np.log(1. - self.exploration.value(self.num_timesteps) +\n",
    "                                self.exploration.value(self.num_timesteps) / float(self.env.action_space.n))\n",
    "                    kwargs['reset'] = reset\n",
    "                    kwargs['update_param_noise_threshold'] = update_param_noise_threshold\n",
    "                    kwargs['update_param_noise_scale'] = True\n",
    "                with self.sess.as_default():\n",
    "                    action = self.act(np.array(obs)[None], update_eps=update_eps, **kwargs)[0]\n",
    "                env_action = action\n",
    "                reset = False\n",
    "                new_obs, rew, done, _ = self.env.step(env_action)\n",
    "                # Store transition in the replay buffer.\n",
    "                self.replay_buffer.add(obs, action, rew, new_obs, float(done))\n",
    "                obs = new_obs\n",
    "\n",
    "                if writer is not None:\n",
    "                    ep_rew = np.array([rew]).reshape((1, -1))\n",
    "                    ep_done = np.array([done]).reshape((1, -1))\n",
    "                    self.episode_reward = total_episode_reward_logger(self.episode_reward, ep_rew, ep_done, writer,\n",
    "                                                                      self.num_timesteps)\n",
    "\n",
    "                episode_rewards[-1] += rew\n",
    "                if done:\n",
    "                    if not isinstance(self.env, VecEnv):\n",
    "                        obs = self.env.reset()\n",
    "                    episode_rewards.append(0.0)\n",
    "                    reset = True\n",
    "\n",
    "                if self.num_timesteps > self.learning_starts and self.num_timesteps % self.train_freq == 0:\n",
    "                    # Minimize the error in Bellman's equation on a batch sampled from replay buffer.\n",
    "                    if self.prioritized_replay:\n",
    "                        experience = self.replay_buffer.sample(self.batch_size,\n",
    "                                                               beta=self.beta_schedule.value(self.num_timesteps))\n",
    "                        (obses_t, actions, rewards, obses_tp1, dones, weights, batch_idxes) = experience\n",
    "                    else:\n",
    "                        obses_t, actions, rewards, obses_tp1, dones = self.replay_buffer.sample(self.batch_size)\n",
    "                        weights, batch_idxes = np.ones_like(rewards), None\n",
    "\n",
    "                    if writer is not None:\n",
    "                        # run loss backprop with summary, but once every 100 steps save the metadata\n",
    "                        # (memory, compute time, ...)\n",
    "                        if (1 + self.num_timesteps) % 100 == 0:\n",
    "                            run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "                            run_metadata = tf.RunMetadata()\n",
    "                            summary, td_errors = self._train_step(obses_t, actions, rewards, obses_tp1, obses_tp1,\n",
    "                                                                  dones, weights, sess=self.sess, options=run_options,\n",
    "                                                                  run_metadata=run_metadata)\n",
    "                            writer.add_run_metadata(run_metadata, 'step%d' % self.num_timesteps)\n",
    "                        else:\n",
    "                            summary, td_errors = self._train_step(obses_t, actions, rewards, obses_tp1, obses_tp1,\n",
    "                                                                  dones, weights, sess=self.sess)\n",
    "                        writer.add_summary(summary, self.num_timesteps)\n",
    "                    else:\n",
    "                        _, td_errors = self._train_step(obses_t, actions, rewards, obses_tp1, obses_tp1, dones, weights,\n",
    "                                                        sess=self.sess)\n",
    "\n",
    "                    if self.prioritized_replay:\n",
    "                        new_priorities = np.abs(td_errors) + self.prioritized_replay_eps\n",
    "                        self.replay_buffer.update_priorities(batch_idxes, new_priorities)\n",
    "\n",
    "                if self.num_timesteps > self.learning_starts and \\\n",
    "                        self.num_timesteps % self.target_network_update_freq == 0:\n",
    "                    # Update target network periodically.\n",
    "                    self.update_target(sess=self.sess)\n",
    "\n",
    "                if len(episode_rewards[-101:-1]) == 0:\n",
    "                    mean_100ep_reward = -np.inf\n",
    "                else:\n",
    "                    mean_100ep_reward = round(float(np.mean(episode_rewards[-101:-1])), 1)\n",
    "\n",
    "                num_episodes = len(episode_rewards)\n",
    "                if self.verbose >= 1 and done and log_interval is not None and len(episode_rewards) % log_interval == 0:\n",
    "                    logger.record_tabular(\"steps\", self.num_timesteps)\n",
    "                    logger.record_tabular(\"episodes\", num_episodes)\n",
    "                    logger.record_tabular(\"mean 100 episode reward\", mean_100ep_reward)\n",
    "                    logger.record_tabular(\"% time spent exploring\",\n",
    "                                          int(100 * self.exploration.value(self.num_timesteps)))\n",
    "                    logger.dump_tabular()\n",
    "\n",
    "                self.num_timesteps += 1\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, observation, state=None, mask=None, deterministic=True):\n",
    "        observation = np.array(observation)\n",
    "        vectorized_env = self._is_vectorized_observation(observation, self.observation_space)\n",
    "\n",
    "        observation = observation.reshape((-1,) + self.observation_space.shape)\n",
    "        with self.sess.as_default():\n",
    "            actions, _, _ = self.step_model.step(observation, deterministic=deterministic)\n",
    "\n",
    "        if not vectorized_env:\n",
    "            actions = actions[0]\n",
    "\n",
    "        return actions, None\n",
    "    \n",
    "    def action_probability(self, observation, state=None, mask=None, actions=None):\n",
    "        observation = np.array(observation)\n",
    "        vectorized_env = self._is_vectorized_observation(observation, self.observation_space)\n",
    "\n",
    "        observation = observation.reshape((-1,) + self.observation_space.shape)\n",
    "        actions_proba = self.proba_step(observation, state, mask)\n",
    "\n",
    "        if actions is not None:  # comparing the action distribution, to given actions\n",
    "            actions = np.array([actions])\n",
    "            assert isinstance(self.action_space, gym.spaces.Discrete)\n",
    "            actions = actions.reshape((-1,))\n",
    "            assert observation.shape[0] == actions.shape[0], \"Error: batch sizes differ for actions and observations.\"\n",
    "            actions_proba = actions_proba[np.arange(actions.shape[0]), actions]\n",
    "            # normalize action proba shape\n",
    "            actions_proba = actions_proba.reshape((-1, 1))\n",
    "\n",
    "        if not vectorized_env:\n",
    "            if state is not None:\n",
    "                raise ValueError(\"Error: The environment must be vectorized when using recurrent policies.\")\n",
    "            actions_proba = actions_proba[0]\n",
    "\n",
    "        return actions_proba\n",
    "    \n",
    "    def save(self, save_path):\n",
    "        # params\n",
    "        data = {\n",
    "            \"checkpoint_path\": self.checkpoint_path,\n",
    "            \"param_noise\": self.param_noise,\n",
    "            \"learning_starts\": self.learning_starts,\n",
    "            \"train_freq\": self.train_freq,\n",
    "            \"prioritized_replay\": self.prioritized_replay,\n",
    "            \"prioritized_replay_eps\": self.prioritized_replay_eps,\n",
    "            \"batch_size\": self.batch_size,\n",
    "            \"target_network_update_freq\": self.target_network_update_freq,\n",
    "            \"checkpoint_freq\": self.checkpoint_freq,\n",
    "            \"prioritized_replay_alpha\": self.prioritized_replay_alpha,\n",
    "            \"prioritized_replay_beta0\": self.prioritized_replay_beta0,\n",
    "            \"prioritized_replay_beta_iters\": self.prioritized_replay_beta_iters,\n",
    "            \"exploration_final_eps\": self.exploration_final_eps,\n",
    "            \"exploration_fraction\": self.exploration_fraction,\n",
    "            \"learning_rate\": self.learning_rate,\n",
    "            \"gamma\": self.gamma,\n",
    "            \"verbose\": self.verbose,\n",
    "            \"observation_space\": self.observation_space,\n",
    "            \"action_space\": self.action_space,\n",
    "            \"policy\": self.policy,\n",
    "            \"n_envs\": self.n_envs,\n",
    "            \"_vectorize_action\": self._vectorize_action,\n",
    "            \"policy_kwargs\": self.policy_kwargs\n",
    "        }\n",
    "\n",
    "        params = self.sess.run(self.params)\n",
    "\n",
    "        self._save_to_file(save_path, data=data, params=params)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, load_path, env=None, **kwargs):\n",
    "        data, params = cls._load_from_file(load_path)\n",
    "\n",
    "        if 'policy_kwargs' in kwargs and kwargs['policy_kwargs'] != data['policy_kwargs']:\n",
    "            raise ValueError(\"The specified policy kwargs do not equal the stored policy kwargs. \"\n",
    "                             \"Stored kwargs: {}, specified kwargs: {}\".format(data['policy_kwargs'],\n",
    "                                                                              kwargs['policy_kwargs']))\n",
    "\n",
    "        model = cls(policy=data[\"policy\"], env=env, _init_setup_model=False)\n",
    "        model.__dict__.update(data)\n",
    "        model.__dict__.update(kwargs)\n",
    "        model.set_env(env)\n",
    "        model.setup_model()\n",
    "\n",
    "        restores = []\n",
    "        for param, loaded_p in zip(model.params, params):\n",
    "            restores.append(param.assign(loaded_p))\n",
    "        model.sess.run(restores)\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(values, window):\n",
    "    \"\"\"\n",
    "    Smooth values by doing a moving average\n",
    "    :param values: (numpy array)\n",
    "    :param window: (int)\n",
    "    :return: (numpy array)\n",
    "    \"\"\"\n",
    "    weights = np.repeat(1.0, window) / window\n",
    "    return np.convolve(values, weights, 'valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(log_folder, title='Learning Curve'):\n",
    "    \"\"\"\n",
    "    plot the results\n",
    "\n",
    "    :param log_folder: (str) the save location of the results to plot\n",
    "    :param title: (str) the title of the task to plot\n",
    "    \"\"\"\n",
    "    x, y = ts2xy(load_results(log_folder), 'timesteps')\n",
    "    y = moving_average(y, window=50)\n",
    "    # Truncate x\n",
    "    x = x[len(x) - len(y):]\n",
    "\n",
    "    fig = plt.figure(title)\n",
    "    plt.plot(x, y)\n",
    "    plt.xlabel('Number of Timesteps')\n",
    "    plt.ylabel('Rewards')\n",
    "    plt.title(title + \" Smoothed\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEWCAYAAABliCz2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAIABJREFUeJzsnXeYG9XVh39H0jZ73b3uDXcbGxtY3GgGY4pNT+glhNATIHwhAZJQQnUoCQmB0Duhh0BwKDYYjI2xWYMbLrite1n3ddmq8/0xc6U7oxlptKuRtLvnfZ59djTl3quRdO6Zc08hZoYgCILQ+AlkegCCIAhCehCBLwiC0EQQgS8IgtBEEIEvCILQRBCBLwiC0EQQgS8IgtBEEIEvZAQi+oiIfpbpcQgGRHQZEc1IUVu9iIiJKJSK9oTUIQK/iUFEpUR0QqbHwcynMPNLfrRNRC2J6FEiWktEe4lopfm6vR/9JTm2o4joayLaTUQ7iGgmER2R5jGIQG6iiMAXUk4mBQkR5QL4DMDBAE4G0BLAaADbAIyoQ3spey9E1BLAhwAeA9AWQFcAfwJQmao+BCEeIvCFCER0KhHNI6JdphZ6iHbsVlNTLieixUR0lnbsMlNT/SsRbQdwlzIRENHDRLSTiFYT0SnaNV8Q0RXa9fHOPYiIppt9TyWix4noVZe3cSmAHgDOYubFzBxm5q3MfC8z/89sj4mor9b+i0R0r7k9lojWE9EtRLQZwAtEtISITtXODxFRGREdZr4eZd6vXUQ0n4jGuoytPwAw8+vMXMvMB5j5U2Ze4HAfdxHRKiIaY+5fR0RbdTMYEbUiopfNsawhoj8SUcA8FjBfrzGve5mIWpmXTjf/7zKfgEZrbbp9Bq2I6Dki2kREG4joXiIKmseC5nXbiGgVgIku71/IMCLwBQAAER0K4HkAVwNoB+ApAB8QUZ55ykoARwNoBUMrfZWIOmtNjASwCkBHAPdp+5YBaA/gQQDPERG5DCHeuf8CMMcc110ALonzVk4A8DEz7038rl3pBEMD7wngKgCvA7hAO34SgG3M/B0RdQUwGcC95jU3A3iXiIoc2v0RQC0RvUREpxBRG4dzRgJYAOO9/gvAGwCOANAXwMUA/kFEhea5j8H4PHoDOBbGZPdz89hl5t9x5vFCAP8wjx1j/m/NzIXMPEvr2+0zeBFAjTmOQwGcCOAK89iVAE419xcD+KnD+xKyAWaWvyb0B6AUwAkO+/8J4B7bvmUAjnVpZx6AM8ztywCstR2/DMAK7XUzAAygk/n6CwBXJDoXhrZeA6CZdvxVAK+6jGsKgEkJ7gED6Ku9fhHAveb2WABVAPK1430BlKsxAHgNwB3m9i0AXrG1/wmAn7n0Pcjsb735vj4A0FG7D8u1c4eaY+2o7dsOYDiAoDnOwdqxqwF8YW5/BuA67dgAANUAQgB6me2GvHxeMCbxSgAF2vELAEwztz8HcI127ER7+/KXHX+i4QuKngB+Y5oSdhHRLgDdAXQBACK6VDP37AIwBIYmqFjn0OZmtcHM+83NQofz4p3bBcAObZ9bX4rtADrHOe6FMmau0MazAsASAKcRUTMAp8PQvgHjvp1ju29HuY2BmZcw82XM3A3GPewC4FHtlC3a9gHzGvu+Qhj3PgfAGu3YGhjrAjDbtR8LwRDebrh9Bj3NvjZp7/EpAB20vvTPRO9XyCJklV5QrANwHzPfZz9ARD0BPANgHIBZzFxLRPMA6OYZv9KubgLQloiaaUKoe5zzpwK4l4iaM/M+l3P2w9BgFZ1gaNwKp/eizDoBAIvNSQAw7tsrzHxlgvcRAzMvJaIXYWjmybINhsbeE8Bic18PABvM7Y3mMWjHamBMKF2RHOtgaPjtmbnG4fgmWD+THkm2L6QJ0fCbJjlElK/9hWAI9GuIaCQZNCeiiUTUAkBzGEKwDACI6OcwtFPfYeY1AEpgLATnmguMp8W55BUYAupdIhpoLl62I6LfE9EE85x5AC40FxtPhmH/TsQbMEwV1yKq3QOGeek0IjrJbC/fXPjtZm/AHM9v1DEi6g5jEvnGQ/8WmLkWwFsA7iOiFuak/H/meABjgrqJjAXvQgD3A3jTFNhlAMIwbPte+toE4FMAj5Dh8hogoj5EpO7bWwBuIKJu5rrErcm+HyE9iMBvmvwPhmlA/d3FzCUwFt/+AWAngBUw7Lpg5sUAHgEwC4aGOBTAzDSO9yIYrpXbYSyOvgkXV0ZmroSxcLsUhj1/D4wF3/YAZpun3Qhj0thltv2fRAMwhd4sAGPM/tX+dQDOAPB7GIJ0HYDfwvm3VQ5jYXQ2Ee2DIegXAfhNov5duB7APhiL5TNgTETPm8eehzH5TQewGkCFeb4y19wHYKZpohnloa9LAeTCeJrYCeAdRM1Wz8BYt5gP4DsA/67j+xF8hpilAIrQsCCiNwEsZeY7Mz0WQWhIiIYvZD1EdIRpQgiYJpgz4EErFwTBiizaCg2BTjDMBO1gLK5ey8zfZ3ZIgtDwEJOOIAhCE0FMOoIgCE2ErDLptG/fnnv16pXpYQiCIDQY5s6du42ZnVJ5xJBVAr9Xr14oKSnJ9DAEQRAaDETkObJZTDqCIAhNBBH4giAITQQR+IIgCE0EXwU+Ed1IRIuI6Aci+rWffQmCIAjx8U3gE9EQGLlZRgAYBuBU0qoMCYIgCOnFTw1/EIDZzLzfzND3JYCzfexPEARBiIOfAn8RgKPN1LTNAEyAQx5zIrqKiEqIqKSsrMzH4QiCIDRtfBP4zLwEwJ9h5NH+GEYO8lqH855m5mJmLi4q8hQ7IAiCkHY+X7oFG3cdyPQw6oWvi7bM/BwzH87Mx8DIof2jn/0JgiD4xeUvluD0f6SzDETq8TXSlog6MPNWIuoBw37vpdCCIAhCVrFjXxUAYNtex7o7DQa/Uyu8S0TtYNTe/CUz7/K5P0EQhJSzcMPuTA8hJfgq8Jn5aD/bFwRBSAd//mhppoeQEiTSVhAEIQGLN+3J9BBSggh8QRCEBOSGGoeobBzvQhAEwUdqasOZHkJKEIEvCIKQgHAjqQQrAl8QBKGJIAJfEAQhAWcd2jXTQ0gJIvAFQRASkBtsHKKycbwLQRAEH6nlxmHEF4EvCIKQgABlegSpQQS+IAhCAnq2aw4AOLJvuwyPpH6IwBcEQUjAQ58sAwB0almQ4ZHUDxH4giAIHmnoph0R+IIgCB4JUMOW+CLwBUEQmggi8AVBEJoIvgp8IrqJiH4gokVE9DoR5fvZnyAIgp8wGrY/vm8Cn4i6ArgBQDEzDwEQBHC+X/0JgiBkOxXVtRnt32+TTghAARGFADQDsNHn/gRBEHyjPgG3ny/dgoG3f4wF6zNX6dU3gc/MGwA8DGAtgE0AdjPzp/bziOgqIiohopKysjK/hiMIgpBRpv+4DQAwd83OjI3BT5NOGwBnADgIQBcAzYnoYvt5zPw0Mxczc3FRUZFfwxEEQag39bHgK5fO2gwm1/fTpHMCgNXMXMbM1QD+DWCMj/0JgiBkLSpoK5N52PwU+GsBjCKiZkREAMYBWOJjf4IgCL4Sroe0VjFb9Wmjvvhpw58N4B0A3wFYaPb1tF/9CYIg+IFez7a6tu7CWpl0MunY6auXDjPfycwDmXkIM1/CzJV+9icIgpBqajSb+3/n18PRsJGbdARBEJoUSzfvwe791Y7HlKAPELB44x7s2l+VxpGZfae9R0EQhDSxe3815q+rn997Mjb3kx/9Cuc+NcvxGJvthBmY8PevcPmL39ZrXHVBBL4gCI2WS56fjTMenxkRtnXB7kUZTuBWuWxLueN+NYRtew3L9rx6TkR1QQS+IAiNlgXrdwMAznh8Zp3bsGv4lTVhlzPjo1pRfvg5GSiMLgJfEIRGS27IEHFK8NcFtsn3uiZQC0dMOsb/TKTWF4EvCEKjJZSCElV2Dd/NOpTIbKQOKw0/E8VUROALgtBo2V9V/+yUdjHuJtYTLRPoi7ZAxEszrYjAFwRBiEOshu8s2RN586ijYdHwBUEQspMYge96Xvx2VDNiwxcEQchSvHp0JtLw1fFa838gBesLySICXxAEIQ5KUDfPDQKITgAV1bWo1vLsJBL4izftsVwvJh1BEIQsQ5lqurdtZmyYrwfe/jFOfnR65LxEC8TfrzUCraJeOqkdpxdE4AuC0OiorKnFjn3ectVs31uJqjjBVByxuatsl1FNfmXZvsh28b1TPfX3gZmAbdve9OfSCaW9R0EQBB85UFWL4nunYF9VLbq0ysfG3RVxzz/83qk4ZUgn/PPiwwEAb5esQ24ogDOGdwUQNcGowNhMZrusLyLwBUFoVAy64+PINiWwk6v6sh8t2hzZ99t3FgBAROAr23wwxfnsh3ZtlaKWvONnTdsBRDRP+9tDRL/2qz9BEAQ7G3YdiHv8J//8OmEbEZu7aXSvTyI2nVAw/UZ83zR8Zl4GYDgAEFEQwAYA7/nVnyAIjYtteytxoKo2uliaId6fZ9jc1aJrXcS90xpBTiD9S6jpMumMA7CSmdekqT9BEBo4ahG0dNJET+c/N2M1VpXtTfk47GmMmZF08ZIVW2PHFcyAm066BP75AF53OkBEVwG4CgB69OiRpuEIgtDYuOfDxb60++WPZTH7DlTXP0dPJkw6vj9TEFEugNMBvO10nJmfZuZiZi4uKiryeziCIDRyurct8LX9uqZHzgbSYUQ6BcB3zLwlDX0JgtDI8OpPr1i3I/5Cbb3h1Lhm/rBxT/0bSZJ0CPwL4GLOEQRBSMS8dTvT1pcXDxxG1HOnPiQ7kaUCXwU+ETUHMB7Av/3sRxCExkswjd4sTnK8fWGu5TUzLDl0vJCJzJhO+HonmXkfM7dj5rrXFxMEoUkTTIG0zPVYP7YmHCvIC/Osvi0MjtHw7U8GHy7YaHnt9BYO6daIAq8EQRBSwZsl6+rdRqJMlpHzNHlvr1Cl2LW/GtW11p32c/7+2XLLa3Kob5WJFA0i8AVByGr+O39j4pMS4FXg6xq+EuLjBnUAAAzo2AIAcPPb82M0fHv79uNOGn4mvH0kl44gCI0etzVWIqumHavhE7q0Mtw8S7cbmTF/2LjHMjFUOPjkZ2uCNdHwBUFoEtjt7MwcI5idNHylvVdq6RFqtBmkqjYc046XJwox6QiCIPiEXcuvcVD7dVOMMrmoPRMP6Ry9VrPh19YyXppVGrcvpwlABL4gCIIDW/dUIFxP3/d4dvaurQ2zTa12jtpU/w/t3jpyTH8SqAkzJn201NK23T7vJNwzYfURgS8IQtYz4v7PcM/k+uXKsQv83QeqI9vK3KNr7uGIl44qSRhdedWfDpyCsOzenc4afvpFvgh8QRAaBC/MLHU99tAnS12PKZiBvZU1WLa5HI9PW4GR938WPWb+Dzto+Ao9u6U+MTj57tsFfLYs4oqXjiDASHe7r6o28mgvZJZktd/Hp61MeE6YGac/NgOrtu1zPAZYNXe1T42lIDcYOVarCXlHDd82fvvrPkXNMzIJiMAXBADD754CwHvudcFfUpCqxrFNJ2EPRDXwA1VRF8uo1m/8P7hLy8ixaouGHzvYSlvBE/spiUov+oWYdARByDqczCTJ0K9DIf52/nDLvniukurIHe8viu4zh6AuG9gpKvA3a4XRnTT8RKkXApSZwCsR+IIgZJSd+6ow31ZVyi0bpa6Bx+Osw7ri2P7W+hrsMof071gYEcjfrY2OI+qWafzXdfL7/rcksl1TGzvWWL986+sAkbhlCkIm0L01MpGytqlz7lOzcMbjMy37tpU7fw61SUjJZrmGxbqFmfzMTcPv0ba5owlJ7VOXuVlhnNpNpOED4pYpCBlBD42vSTLtrVB/lpv1XnWheNkLczxf/9736x3354YCWP3ABNx80gAA7gI/N0SOx8orqi3jcrO7O6VKtk9Mzhq+mHQEIe14TayVDlZv24det07G4gxUQ/Kbjxdtwp6KatfjulB0W1ytrokVrje9OT9mX46ZQ5+IEDDdKd0WgnOCAUcT0k1vzgNgaOLx1lirHMZkDxKLseEHRMMXhIyg/9idPC7SyWdLjEqg78x11lobKqXb9uGaV7/D/zkIZ4WXoiIVNd5s+LrPvNp006hzggHHKF416TBbg67s2FMlA05umdbjzXJCGZH4fle8ak1E7xDRUiJaQkSj/exPEOqC/tvU7fmZIC/H8PX2KtgaCspNcc12Z80dcF6oDQWsgjbMwMwV2/DDxvg1lawC34OGHyfXTZg5smA7YWgnAMDo3u0i51XVxn5W9r7sk03HVvnxhu8bfmv4fwPwMTMPBDAMwJIE5wtCRvnNW+4aaDrIUeaHDD9ppBrl6RJPU3bydsnPCVpeMzMuenY2Jv59Rtz+Ag4avpvpLi8UiEmFoPoyxh416Qw38+nok5OTSceO08fZqEw6RNQKwDEAngMAZq5i5l3xrxKE9KPLgXgaaDrJomWFlHCm6YUTzxbu5HufE7ReoN+XV2aVurY1bmCHyDZFNHznmxoKOC/a7q2sifSp2lCVq2rC4ch7qXKYqOzY2yc0vlw6BwEoA/ACEX1PRM+aRc0tENFVRFRCRCVlZWU+DkdoStTUhh0LUzhRrQmafR79vP1CaYJprNudFiqqjXscDMTR8B3U4D0VNZbXuuC8/f0fHNu5/6yh6KKlyFA9rjC9geyUV9SgJswxAjjqlhk16SghX8uGKQgwMnkmwm4yImpkGj6MtA2HAfgnMx8KYB+AW+0nMfPTzFzMzMVFRUX2w4JQJy58djYG3v6xp3P1xcKzD+vq15A8URWx3Wcm9N5v7Hb6LZqwdHRvjPF2SdzHiQd3tLxesqkcAHDZC986nq9q5s5ZvcPxuG7SUSap2nA4Yiq6d3JiS/UVL5VYXhsafsLLUo6fAn89gPXMPNt8/Q6MCUAQfMftx+uEbjvuluHkaXf910gBnBtsnAJ/6WZD+FZU12JfZQ1enlUaOVYbZpRXVKMyzoJ1osCra8f2QfvCPMu+BevdLcn6E4fbgv2+yprIE4o6vaaW465HAFE/fsCpxi01rpq2zLyZiNYR0QBmXgZgHID6JbQWhCTZsqcCHVvG94io0jTL6gwulupCIRTMDpvOS1+Xol+HQozp2z4l7Smb/NEPTkNZeaXl2MqyvTj2oRIM7NTC9fqlprbuRtBBCMeTy/qhXfur0TI/FGNGem322sh21KefHfvSGXrXp3GT8W3ZXel6zC/8/lZdD+A1IloAYDiA+33uT8hCKmtqccs7CzzZOlONnvPcDT2Yxy2HSzrQNdtQlmj4d37wAy58dnbiEz2iFj3twh4ALn/RMHss3VyO0b3boUV+rD66dLM1IM2+TuO0RBCKsyCirxv87t0FMcLejmq+JsxxJ5JEvPf9BlTVhrF2+/66N1IHfBX4zDzPtM8fwsxnMvNOP/sTspPPl2zFmyXr8KcPs/MBTw+c8RL84xe6ex9lmQ2/PE6EbCJ2749eO1hLMRyP2jBb0hErtu215tiJsY07SOFUTp4UseGzxfWzrqzf1YgEviAA0cdgL/7KmWB/VVSrc/IFTxf6/Smw+Z+ngk27D2D73rqZEezJzZLh2RmrIttj+rSzHDt9WBfHa2rC4YgXjM7rc9ZaXs9Ysc3y2smu7tSOzoM/PSTucaf2a8OJTTpe0NMspwMR+ILvqJ9FurwSlP+0V8o0IZhJDb/SYlpK/ThGP/A5Dr93qmWC88qqstTEJ2zZY51w2jbPdTyvNsxxXTjdcLok0eTZv6P7moFb+zW1nJIiJv+YtqLebSSDCHyh0fHlsuTiOXI1DTCTTyH6ZONnTp8b35jnW9uJePc7a44gt1iJmjAjFCBMHNo5qfadzCyXHdkr7jXJaOrPfGU8rWzeU+E4Kf/f+P6W16vK9mLrngqcOdz5SSbdEdWeBD4R9SGiPHN7LBHdQESt/R2a0FhIt5Ek2eyX6vxmucGY0nTpRB+3nwJ/7prMLqXpAU5uAl9p+H88dVBSbY/q3S5mXyINPxlFfaX2pOP0GdnXC45/5EuMuP8ztCrIQetmOfjXFSPxxEVR7/R0+wh41fDfBVBLRH0BPA2gO4B/+TYqoVGhPF+2pMlLp2VBTlLnqx+uV4H/8CfL0OvWyXUaWzx0a5KXtYSK6lqMuv+zSIZNr+Sm2eVTN33k5wQsQm7NDudFS0PDD0TMOqqISTweOHsoDu/ZxqH/+Ncl8qfvXdQcEw8xnjTeujqa/9HJ/Of2tBA2M26O6dseE7SnlnSn5vb6yYeZuQbAWQAeY+bfAkjuWUtosigzycIN8TMcpopmuckteKoJqSA3aPHJd0PZXXemuDqWnkvGiw1/464D2LynAvdo3k9PfLECpz0WP7FYXk6aBb62fXCXVhYh9/1a56AopeG3zDcm72vG9kH7Qmd7PwAc278IF4zo4dJ/fIGeaK2AOSrI9e+WCsby0laY2XF94cg+qYlv8IrXT76aiC4A8DMAH5r7klOjhCaLV7t4OMz4z/cb6l11yqunTa9bJ+PGN76PnN8sJ4RKj/l3AOCAx3MPv2cKnvgi8eKcLuOTCQDTNegHP16WcGJNt4avM3fNTk+L9zXhMEIBQn5OEKWTJuKXx/WN608fz2zTra179PQRvdo4CmLFpI+WWoS1/jQwuHOs26ibwJ+8cJNjPd4hXVti2rKtaXv69frJ/xzAaAD3MfNqIjoIwCv+DUtoTFR6FOD/mbcBv35zHp6dsbpe/dmzLjrZidUk9P68jRGNU2n4M1dsw8oy50RbOl6DtLbvq8KDHy+L2T93zQ70unVyJPhGTxtQq01apz72laMJqa7GgOUuScRSQW2Y8ea3ay2T9t8+W245J9GEdHCXlqipjfXSiaeJx0vHoJ4SnLj5xAFx/emf/HIl1mzfHxH0usWmu8NE4jbGXfurHRPzPfTJMvz8hW89BQimAk8Cn5kXM/MNzPy6+Xo1M//Z36EJjQWlNR/UPiZZqgUVfbnNIQpTZ/3O/fho4SbX4/bFtEkfLY05Z/u+aB/q/IKcIKprw7jo2dkY98iXccfg1E+yvDN3AwDgqxWGV5E+gXxbGs0FtGiDc7lD5eHhJGPGPjQtqbFU14bx8qxSi6D+y5Qfk2oDAN74di1ueXchXvy61PUcpyhbnQCRYcO3LYDqr7vach4lSp/hxJMXH46RvdsltOEDWnpk7VQnU5GXtnQSRfammrgrIUS0EHEUCWb2HrEgNFmUXTyRrVRpuInOO/Pxr7Ftb6VrnhK7SafMIdho9AOfR/s1BWd+TgA793sX4l5MT/FynqsEacrHXRf4bjVddeLdr9Lt+7G1vAIdWsQKwkN7xDrYPT9jNR74aCkIwCWjewEA/m7TzL2ghPmeOJXD4mnjgDH51JqLtjr6+xzTpx3e1spA1sUnXlXT8uLu72TScbIw1SV2IJ0k0vBPBXAagI/Nv4vMv48A/M/foQmNBWU+SeTvrGRjoh/vNlOAuwVJJRu0pCaIfFPDVyTK/eNFw49n9gmaEuM504SVbB4f9TbdtMofN0dNN3rb9ictZsYD5lNQeZJBa3ae+8p4L/vj1BVIFMRVE2ZU1YRjhKde7tB+p+oiZ9UTg9P9G21z74yYdLR9Tt/TvFB2hzbFHR0zr2HmNQDGM/PvmHmh+XcLgBPTM0ShoaMEfiIXtOrIk4D7ObpW3e8PH0Wu0e30Md4TDKzdvh8vzHReG1Ca8oHqWouf9bqdB+KO18vicLxJ4XnbeNT9yc8JRErpxUOd7ybw9clL37bbtHVX1GRMErWmYNZRE8ZOM3+OU8ph+2K3fRF5X2UN9lbWxJiFgppKvXxLue2Y93EPMhdb1ROE00RrT12sutZvj9O9OtJjVtF3rslMeW+v0xER0ZHaizFJXCtkmJ37qvDQJ0t9zQQZr/i3SnXglMu8qiZamerRqYYJId6TwI9brAuOO/dVYenmPRh4+8eYvMCw6//6TSOSVGlbNeEwznpiJv7038XYZ9Ngh3VvjVrTI+QLW4RuqwT+/E6mib2VNZboSa92/q17KiKfT14o6Fjuz45qWxd2bZpFx7xHS3imC3z7xGsV+M592XPMA8BFz36D/n/8yPH85nmG18xftXUA5TZpt7/rWvHR/dpjr4tdW2n4fTsUYv5668JvMhOVysKp7pv9e9kyPxQzkZWVGy64ulbvdK/aNHN3HdUp7tUWAFwjcP3Cq9C+HMATRFRKRKUAnjD3CQ2Auz9cjMenrcTnS7f60v6KreUY9qdP8bZZOcjOy7PWAHDWpI57+AsMvP1j3PWBc7k6O5t2W7XuDxdswoJ1xo9/2jLr+1MFsGtqGdv3qR+stb0gGYLTyVPD6YlEf8K45tW5lmOrt+3DkDs/wSNToh45ureNvli5YqtVQ123c79lLcHT04M5FmvBbtKOR9tYoAnIGIGvadxugtPpXnyzyr3ITKdWxtqBvobx08O7AYj9DPS4gLxQMOLV9RtbmgIloAd0aoHTbEnXkjHhqwlZmXTsl7I2fsVUM7hNP1e/Vw/+5BB8eP1RyE3CpNOqICdhYrdUk7A3IgoA6MvMwwAMAzCMmYcz83e+j05ICUqD9itPzHrT9PH+vI1xz3MS+Bt2Gdfqj+/xin/MWrnd8nrasq2RNuwFr/NNQbJRy0hol1sBIoTNvC12lMDcW1mDP/5nIfZX1WCa9hRgT9Wrniwen7Yy2oamqX+8KOpZdN5T38T0tdGczEKBgKckbkrDn78uGryka6D6Pf1hY1Tg25vWTWBu6yfqs2PmmElX53izeHjPtsY6QXMtQlZ9PvbvYV4o6kOfnxOIHG/VzPqEpT6jUIDw3/nW71oy+XBqwlZTmP6EdMqQTnjm0mKL+UhHvz96l4X5IQzp2gqAUXXLC8EApT1ZX0KBz8xhAL8zt3czs+dwSfOJYCERzSOiksRXCH6gvth+hXHnaqaTeHg1KcUzpdh99Nfu2B/x81aP3QolOPRc7mG2FqsOM2Pj7grHyVDdr6e/XIlXv1mLF2aWxv2BVjgsVOrvWTdD2BdHv1q+DX94bxEAYxL0Ygpyegqo0sxMur+77rZo9xyq0AuvuKSyVlrxOU/OwugHPse8dc4Rskp2qnunL94qwVptG7euFesBVG5++E72+mRy06t8Ox1aGGaqbm2a4exDu+Kavv+9AAAgAElEQVTD64/CPy8+HKN6t3P1rtK7IRBGmKYZXdu/5eSBOKZ/4vrcwQB5iuxOJV6fJ6YS0c1E1J2I2qo/j9ceZz4RFNd1kEL90Muy1ZeXvi7Fe99bMx4qf+REMmrT7oq4tn5FMv7ta7SKQVNtOWXUwuF6bfH16emrsGu/tdbo5AWbHPtUbddo2q1bsi8AWLYltvyeHjGrxw7YJw57mlwvJp1qSyoGNUbnc3u0bRZzrkJ/TwVm6gD7OcrOXWImXtMXwMNhxic/bEavWydj6hLDrBYV+MbE1rFlXmSR1O7Oqtvw83WBb9PalQnG6WksGRv+zSf2x5e/HYvu5j0JBgh/OW94REMH3H3Rdd97fRj2IXmZf4JEqKrJzlw65wH4JYDpAOaaf6KxNxCUm+KNb8yrdzrWOz/4ATe9Od+yT2lDTlqRXVN8dGriYJ76plaIx2Ofr7DauePcj6emr7S8rgmzRXDGY8kmI1jqee2JRI+0TDT3Oj0t2e+vvj6wziUJmUJ/m2/PXW9pS9fClTCttvVvv0+6+a4mzLj6Fet6hhL4ahFzwtDOmoZvtK1MbnpAVbO8eBp+IPJe7PLdbs6zo6ctDgUD6NkufhCgW2t6rYUAEeaYAXL2sXrRrYIBwpc/+rOu5obXSNuDHP56e7kUxtPBXCK6yukEIrqKiEqIqKSsLLk85oI3dI12Tz1K1bmhtNjaMGPHviqUakFDdq+Y/ZWJ88+kKjXwxaOck2lZ0hBrQrN3kVUIKH/xr5YbVZUenbrc89hO+dtXePLLlREfe8W/Zq/FT/75ddxr3RZt7V3rk8LYh78AEJueVwl2+ySqm1W2a2sRymPnhRmlkX3XHNsH4TC7muSczFxql2rvpvH9I5PJ3z83nmZyTAGua/K6Pd8uRNWi9ztz1+OYflaTydXHxrebD+nqrbSiwi3fkP5edTO/fQLy8jRt2PCzU8MHEQ0honOJ6FL15+Gyo5h5OIBTAPySiI6xn8DMT5t1b4uLihLbvYTk0bXSRP7KNbVhzFnt7n3hdg1g5IwZ+9C0iPABEJNu+E0XTx6dVLmPHjegg+N+N3fFPkWFlvOUNqfbwpMpgeiU0uH37y1MmI9+wtDOkXHpY5292rpg7SQs7AveqsLUQ59Yc/ms2xl9ItC/E79/byEAYN46Y4xDu7ZCbpBQy+y6fvFXhxQMSuCphePcYCBWY1eBT9r+XG3Csp+vR++OH9wxsj1haCcUJkifnGwkrv3Oqvq6uiAnokgGT3uaBa8avuK84u5Jja+ueC2AcieAx8y/4wA8COD0RNcx8wbz/1YA7wEYUeeRCnVG/5K6fQ+XbS7Hpt0HcM+Hi3HuU7MSCv1vVkWFjxI8YeaY3CB18Qyy53BZvqU84omTDMpjxM7MFdGxq0Wzq47p7cnclWz5xLpQmBeKPEnMXB6t2frlj9YnYCezj12sjXrASMpVYptkxj3yZVT719pRPvK6j38gQGB2j2x2SnZnv5ehAMVowSEHDV+fsOwCX7/+opE9IkKyeW7iXPnJ5rixm8/+cu5wAEDXNtEYgiBRxFPrq+XW2roqcOuIXrH5+RW6YjPMQ6BdKvCq4f8UwDgAm5n55zDcM1vFu4CImhNRC7UNIzJ3UT3GKtQR/bfHLvL3pEenY/QDn+Ml02c+UbHr85/+BqvMjJLqi6s/BqtIyHheCF6F+Pi/TseRkz5PfKINN61OX/RTE1JBThBH9UscJal7/Jx8cKekx+SFgpxg5ElC91F/6stVlvOcNHxG1N89EZ/8sBmA9anlopE9AUQ/01CAIvcrmWpg9mCmYIBihK5qVxfsIZdt/XXfDoUgIgzp1irmejeSTb1gv7OqDz03kb7esOuA1UNMzaFXH+NualqrrbukqxCKV4F/wHTPrCGilgC2wqh6FY+OAGYQ0XwAcwBMZuaP6z5Uoa7o3yWvOdy9BIQobVdpiLp9e/xfpwOIClRVMUjn7CdmOrbrFNXpBadqR04MNh/Pc0OByP0IBQhDu8bVYQBENUVdC081uaFAZNH0ec2WDgAHqmrx0cJNYGZH81Jlda2jecPJTVB5L1lq6ZrbSuAHTA0fSE7gq5w6CqJYgb/ZzFWkC2zdRdN+vppE7jh1sNGm1nYiktfwra+dvLMIFEmRfMVR1iXNSNlMbVKwRxhb+ktqdHXHq8AvMWvYPgPDQ+c7ALPiXcDMq5h5mPl3MDPfV8+xCnVEfzxdsz1xFkYgdvHPCfUjUpqmo1+4KUBOdShGrezLdnq1c/eEaZEfQrGLYO8S5welo4RZl1b5kR9yyMHG7IQSjgW5waSTtHklaJpQwmGOJIpTvPJNKa597Tt8uGCTo0mnojpscW1UHOsg8MMRk070c3vENKepY0GKavjxXFLt6Nk+O7Y0JnC322vV8KMiaa3N82jdDjMwLWh1M/YSrJpsMk27xu3kBfXkl1EvLvskqy7XJ5qj4zxBJhM4Vh+8eulcx8y7mPlJAOMB/Mw07QhZTK9bJ+O4h7+wmFXOe/qbmPOcbNdOGr79S6++zP80qznZv7O791fjnbnGIm2hmb9kWLfEWrQugL5eEbWN9rp1MsoratzD/8MceS+/PqGfa/vKDdHIjmmcnxMkDO/eGhOGdsKH1x8FIDZjYou8EGabaxsFOcGUaPg9HSY3de9XlO1Fnk14K0+VjbsOxJh0asOMqtowCnKCGNW7re2Y8R3Qfd6VB5GTG+ywboZN+eJRPSP3+/MlybsQFrXIi6yl2CNnFQEivHX1aLx33RiLomFPlaHYZ3p6qYnbi7Csr4bvVoGs2vSjzwnZFm1Nnd2SiiGOQpGurMpeF21fIaIriWggM5cy8wK/ByakhtXb9iVcOHV6VHf6ffzshTmW10ozUxkm19uyS978zny8+s1aAIZ9f1i3VmjTPHFyKX0x6+FPYytFOTlJD+jYAjXhcMQUEs8kNemjJQCApZujgVLBAIGI8MRFh2NI11YY3LklmueFLGMpr6zBe98bRUt0OzsALFxft3q9vR2KwiiN+sS/TrekTQCiQqOW2VLLFohq4Pk5AZx3hNXiev//DI+hu04/OLJPPWFVOTyZFZlRqGMHFEXG8+yMVTHn2dEnyV63TkZZeWVEa9ddLgFE8uEEA4QRB7XFoT3aWPzpO7V0fmI7xFQalGPB6u3xYxCAOgh8m5FFeenYUU989tz9W82JWTc3OQWMKdKk4Hs26TwPo2j5Y0S0iojeJaIbfRyXkEIShW9PsUWoumHPY55IK9GfCHJDAYSCzjliVHg6YNg5LULWIXNia4fUC4EAoTYcNSvF+3GVOggIuztjKEjGBOJy7/Jzg5ZxvuXB3dSJ1g7ZFd1yCR3UvnlEcC3ZFBvVqzynCnKDGN49avbSXTudtGGl/RfmhXBecXfc/d/FEXfKUJAiE7ubCU4xtGsrvH7VqJj9dtOMotC0b+vCWJ+of35kL8frVJqIjxYZi87Tf0wcvxMtYJLwVADW+sKA87rSxKGdNbOStWFl4tFTRdgTvmUCryadaQDuA3A7DDt+MYBrfRyXkELsaX/tvDt3fcw+L+Zp3TXTCV2DzgkGkBOkyCOwTltN6+/fsdAiSJ188pdtKcdTlxweeU1kRLZOXbIlKvBNwXHbKQNjrm9n+k631kwM9gkiGKCIicSJ5rnWFMZ1Ne/YNc/fnTzAMWq0TbMc9OtQGHm4sScPA4AXZpYCAPJtmvRabYKzJPwyhZIae35OABU1tXh+5mpsMhPO5QQCronE7PQpco5e/W6tc9yBWmvQ5zddU05UTEQ9hXhBadpe8+bbNfzmubHrImMHFEU84OytRov5GKZAAOhri/PQcapM5gdeTTqfAZgJI8XCMgBHMHPsL0nIGuKV1rPjJFR1t7pd+6sc27v9fW8pjQHjx5sTDMSE7APABSOjEbH5OVZB2s0hlcGa7ftxkuYSqWutUZOOse/yow6KuV75b591aNfIPrvAzwkY0a5OCdEA44lFF/J1fSS/8hjr+AZ1bhljHgAMc0htmPHEFytjjilmmOsdizftQTNNQD2imcX0TzFSp6A26la7ebe1ylcgQJ4WRQE4LhYD1qe0+88aGtkuiAh8XcPXctXYPhN7JPTZhxmfn329wgnVlNcALP0nMensoY5PXYyoecmeFll5grXMz8HBZpSvWsdy4jiXmJFU49WkswBAFYAhAA4BMISIvLlECBlhZZm1UIjdi2DJpj0Y+9A0LNtcjpOGxPqTq8XPGcu3YfjdU/DTJ52dsv421Vvd09xQALkuJh3djp0bClgmoCP7tIs5/54zhwAARhxk/NB1gRE16RhfbWUiGNipBW433flUOT89YMfulRQMGCadr810zHa3xpD5BKCotFfZsvG384c77h/YyWobLirMc/SQsk8wAHDCIGchsWjDbkt2TD2ls93/b/PuikiSudxQILIoreNVw7/qmMTZVi4c2SMiJJUQ1Z9ydMFqf/p595oxkQV1ILomMKp37HfETiQVskeBP6hTCwDAe9eNwfkjnFN0MDP+efHheO+6MZY00ABw75lD8PY1o9GjXTM8dUkx3v/lkWnPfe+EV5POTcx8DICzAWwH8AIA5/yoQlbQzBZ9qDQIFSx06mMzULp9P056dLrj46oSZvPXGx+zWzqAv3pIhgYok07A0aQTsoXT64Jtn4OGrWy/NxzfL3KNQiWz0p9QPr3pGLx51WjY5aibOyBgmjeqw5FUzeM14XrG8C4IkFXgH9M/ftDW6cO6oHTSRJROmhipKWu3Lvz7ujEY0rWV4/pDbigQ4ypYE+aIuUDniqONp4ZTHCZyu6miZM2OSKlFp7UNIL7bY98OheY5hN6myeJno3tazrGbqF69YiQ+uvHoiPAlctbw7XehTfNcS0bLi0b2wNCurXChi0DWUQLfqw3/hnH98O61Y3BoD/fYjqpaRmFeyPGc/JwgjjDXploV5DhG0mai3rlXk86viOhNAN8DOAPGIu4pfg5M8M6nP2yOqTZlF6uhAKF/x8KI6UEXVmrxS0cJzAEdW6RkjMaibbTgg24i0oWtXXP++2exTxARV7igigOIatc3vP49AOAd7X7079gCrZrlIGhKLiXEO2tVjexCNi8URGVNbcQ00ruoMGIKCgYIP2zcY1mjKHAxZwDGorQu1J646LBIOzqHmYLDqZBMXsiaUG1499bYtKvC0eSlBK+9di0Qm4DN6bO3E0/DVzER+mdmN3/YYwBa5udgUOeWkYlEvwu6FpxIGe/YMh//vf4odGiZ2P6t2nJSIJwIBQMJA/lURtS68uovRtbr+rrg9RkjH8BfAAxk5hOY+U/MnHysu+ALV70yF799x+opW2tztQsGCAeqa7Flj9VGCwBTFsd66agfcKqiSZVJRy2CqipLJwzqaBF8wQBFFgzd1iFUG0qwOKUYcBq20ihVTn49bYFdSAWDxsQT8fUOEPp1NATp3oqamLQQ8RK+PWoz56j362ZP/nrltph9+TlBbC2PfnbN84JYtqUcSzbtiZk4lNB0ktP2W+plQg/aXAvf0Dxx7KZDINY89si5zuYsimj42rU+qb32rK314RwzdUW8yFkvjPFY8DyVeDXpPAwgB8AlAEBERUQUuxomZA32XCaTF2zCuh0H8N1ab5a46ohg9h5dGY9cZdIx2y01I37nrN5uMVXsPlAdWQB7anp8v2/98d++oOcU5LJjn9WtUNcm7YJm2eZyrCzbF5nwQgHCa7ONmIJPtQlSvZ94E6M9Alh1pQTpM5cW4/cToj4QRzkIgn2VNZF4B8A6wXRvY21f3b8TBnWEnSpb4XU96MptTUCfUHoXNbfYzFdvi43cLtPcN0f3boeWLouVTh4z+meSqIh8MrRPwqMnEarebTKZU7OFZLJl3gLgNnNXDoBX/RqUUH/sYf9ec+gotpo/2vyc1Cw05QYDyAlR5EeiFpF/P2EQ2mlumQe1bx4JFNPTC+sulDvMguTRgJ4AbjnZ6jR2VN/YhbyHP7WuN1gEvk0rXbF1r6WvYIAcJz+1L5mUznZ78vjBHXGVlmTrnxcfHnONbj4CrE81/7rS6vuuJsJxDgJfXaYifPUKZG5PHFYvGuOe3XyiUVCkc6tYLfffZnAaAPxh4iDXdiOeM5axRz8TpxiFuqJSX3spPZgIZeoZ2i25HPvZgNdf81kw0iHvAwBm3gggNcZdwRfiaZxetPb7/hcbjeqVEwfHCppAgJCjmXQiGTZDAYtAeLvEiAmwpwKeccvxke1Xv1kDIGoKqKwJ44DNNvvTw2Nz+9nTJev3wckVEtDMR4GAY0TyFS8Zhd/enus98EoJULfoT7fiGzr6IrrXHEIAcOGIHrh0dM+IWUJlRwWcTXuAdVL4YaNht75ubF/M+cO4hHboeHb4jbsME5Xutqkm3lRq94rvbx+PZy+tf6XVsQM6YM7vx+H4gbHf82zHq8CvYsOgykAk3bGQxcRzE3xP08AAY+FtUGdnbeVRB7fLR84ZFrfv/i52Yd0tc7tpXlEJsRQqJH25rT6s7laqTEC6wFSpfhVOtmB7OuPPtNwwbsni1KQQCAD5IeVGCJxvpi5Qbox6jn3AON6mWU6Mf7Y+bjdhWBeffj13UIs8d2FZkBvE3WcMSUqg2h0CAGMC79Ai3zU/jiJernoVzfvZ0ujnoN66l9rHydKmea7j51EXvCwUZyNe3/1bRPQUgNZEdCWAqQCe9W9YQn1xykGj7MS3/XuhJUpx2eZyVNbU4i/nDsPrmnnArjUDRuTqGcNjQ8R1+69bhGQoSKioDlvs0Zt2O+fEV1GjOlcfa/h5X2OWs9MFYwubndhJgH+21KrBxrPhXzLKcC1U9yAUCGDyDUcDAL79wwmWBTenTIr5OUHM/v0JWHjXiTHHlIbvNeozHqeb4frnHxF1TSxwcLMFgI9/fXRkO14QkJ1k/cd1V9G2hcmZZVRyOMEfklm0fQfAuwAGALiDmf/u58CE+qEevfubniW3nzoYh3SL+gIP7BTVwssra7CqbB/OPqwbRpuBTsO6tcLVr1oLUwPRnDh2bhjXz3KOE8p+f9zDX6CXWURazy9y3ICiSEyAU3GU204ZhIV3nYgrjjYEv57f5BpbTVMnE80RvawRmXq6Wvv5xWalIuVNFAwQ2jTPRemkiWhXmBfR9gHg6AenxfQ1fnBH5IYCMQnDAC0IyEXgJ1OOT31eidIQAFaPnNMO8Z7X5dqx8evFPvezYrzyi2gxu8u0HDhOcQLx6O6xSLxQNzxP3cw8hZl/y8w3A/iMiC7ych0RBYnoeyL6sM6jbKJs3l2B2Qny1Sze6OwLrMweY/q0x6r7J+DyI3tZsmYqk08Phx9Yvw6FKGqR75iUSgmr5y+z2kL1JQM3gf+Bmf9la3llxLSjtMfVD0zA85cdgUfOjTUXTbkpWgq5heZb3rZ5LlY/MAGrH5gQE2jmZNI53fZkMlwLhrHPD2oC2FtZ7diemxatODKOy53qa3CXxKmiFW4l8JR5zh7p6YQlc2MSWvsYh2hnnXGDOuJorai4XlA83uSlzEr6mkVdi98I3oj7qRNRSyK6jYj+QUQnksGvAKwCcK7HPm4EsKS+A22KjHrgM5z39DeWXCh2Jvz9K8vr2jDjve/Xo63p4dAyP4SAmfpXt9OXrNkROd9ObZgx1SWDppJ79olif1V04c0u8FVSrU1anhZVPEJ5lBAZY2zbPPYH3y+Or7i6rlMrq0016GDS6dAiHzNuOS56TpxI2/8t2gQAeMas3GTXxt3yxqj8LvEoKszDHacOxgNnD014ruL2iYMc95853OgvFbbpj240TD4TbcVqki0Anu9xLCrdRP9O0aRiamL9zfj+SfUpeCPRJ/MKDBPOQgBXAJgG4BwAZzLzGYkaJ6JuACZC7P1Js2B91F/+sc9XeLpmxda96PP7/+GmN+dH7NvnaWHnut1eyfnaMCNAwPXH940cW+XgW62I+rdbhUD/ji0iSax0bfjxCw+LcRsEokWf7fbhVKUb8eIBo/vq223ayhdeRePaTSb2jJSKe84YgtJJE+OOjYhw+VEHJRW4owtd3bVwYx2Ku7sxqHNLlE6a6JhwLhm8Pj20Myd3PWQkECCUTpqI68e5F7AR6k6iT6Y3M1/GzE8BuADAYAAnMfM8j+0/CuB3AFxdRojoKiIqIaKSsrLEea2bCnsOJB8Z+MvXvotsq0VRp1S7OgeqaxFmd40VAB786SGRbSXourS2atT6o7gubCce0jmSyMspVN2umaprE407EU51XYFowq0RvdpaKn3Zha+y96vqWHafcDeN2otpxQs5QYqkLTDSEBj3o1luEE9pfvqJ/Mr//BNvTxH/uPDQyPbhPdugMC+Em06IatljBxj9TEriqSQR6muSpvrdAoBE386IbxQz1xLRemaOjc13gIhOBbCVmecS0Vi385j5aQBPA0BxcbF89PVg2ZZYn/lE2QGV+5uTR47iEK0soZoYmuWGUDppIi569ht0b2M177gtRv5h4iCc/cTXln2xGr7Kj+PPV6FVsxy8cNkRGN69dVzXP6XRq7UGu4BPlXufG8vvm4Api7eg5OUSdG2dj0rTPfTgLi0tTxu92se/9+cd0QPnHZE4uZh9Ml70p5MsrzukMFJVob6a9qRwgn8kEvjDiEitChKAAvM1AWBmjhdqdiSA04loAoxcPC2J6FVmvrjeo24CpOpHYK+J6kaPOIXDdfOF3bTx2hVRcw0hvvfJYQ5ZBcvKKyPZI4HkS9Hp5IWM4KjLj4xvklCZQ+MtvCqBvj9OPny/Ke7ZBs1yg7jm2D6RyakwL2QxRelrD/++bowlIVwyuJmo6sqZw7ugWYKnHXKqVdnEOGFQB0sGUL+J+4kwc52/Bcx8G8xUDKaGf7MIe++4ldZLFq8LaPHc+nTBGM/0o0hmka+trcZtfXzTzz6sK16fsw4Xj0qs0QLGe/n8N8c6HksU7epkcurWJrUlIto0z8Xiu08GAHxrpn0eahMO+u1ymlDj0bdDYSSFRF6CFBq/OKo3Pl9ahuNd8u3YefT8QxOeo+aqpqzhP/uzI9LaX+Yz8guO6AtfnVyi+vTsiV7aiUc8Aadrf/Fy6yg5H09mf2YTsPayeHaB/9fz4kf16tx52sF46+rRkfTAXuhdVOh4fk6CidLJv97tc0oFR/Rqi39dORI3nmD1XnFKgeyVN7Wsl4mCqwZ0aoGSP56Q0lJ8rQuMyX5oV2eXUyH1pEXgM/MXzHxqOvpqLOgLiv07ObslJpOwS6FnZdSJp+Hpx7zkE28RRwj1sQlX+9OALvCHdG2Jsw7tlrA/RX5OMFIFq74kWjR2Sk3Q0edw+zF92sdMiE5ZQb3STltoz0Q1pk6t8vHBr47EfWcNSXvfTZXUuBQIKUdPfqbcHHfsq0Kz3GDErFKRoKyeEyrC1U5u0NlU8+lNx1jMOPGKQtg1/BG9khe++iJzqu3KyZDjsayfzqE9Gp6m+ruTByQdDZtK9OhvwX/EpJOlqDzleVot08PumYJxj3wZOafSzG3+y+OMyMZimzBW5ed03GzkbouQKhHavWcOwQUjeri6OwLAfWcOxcShnTHioLb48rdj8eLlydsn9fElimb1k0CAEhbjKJ00EXeYdXIB4Bf19F/PBNeN7YtLRvfK9DCENCEafpZSbQr5vFDAYt7ZsOsAKmtqkRcKYpXpa1/csy2GddsWEzy0a39VTLtVDil+VT/xuHhUz7jHAaBX++Z43Czf19PlSSIRuonCywKxn3ip9qWPMdmIVEFIN6LhZymqgEleTjDGVj9rpZFf5zoz0CovJ4BggGKE+ba9sQL/4x+ca5i6pQdONxaTToYFvhfOP6I7Tj64U8xitCBkIyLws5TfmTVqw2HGrFXbLcU6np6+CqVa+oPcYAChQMBTYRO3SNB9ldFr6+rL7ZUHf3KI6zHddF7t8jSSTQQChCcvOTxmMVoQshER+FkIM0ciTbebJfYueOabyPGvV27H2Ie/iLzu3LoAc0p3eKpXe7mWulanmWYvn3bz2OQHnQTnFBueN3oEr0LX8O356zPFS5ePSHySIDQAxIafhUxeuClm3/dxhLnX4CrA6jLZvjA3YvYZrGXSVKYUPQI2lRARlt5zsqMroG688iu9QrIU5mWfaem1K0aivCL1VaGExo0I/CxkzfbYCkrxcLN1q+InOnqA1cPnDMNlL3wLINafe8V9p/i6COk25ngl8TJFJnzUExEv374guJF932TBklveC24eNk5CVXe/LIqTECsUDKSkBF+yFOQG8avjjFTNl3jwDPITlQE0GwW+INQF+SZnIc/PKI3Zd/xA5xwmEw/p7Jo+wSlwSRde2SrIxpn5Wi4Y4S0njt/UN1WzIGQL2fmLb+RUVNei162TMeK+qTHH3p+3AacNMyoOffU75+pMOj8xKyz9UauIdMvJRvqErg7JvJTw6tq6IGsF/qE92qB00kQM7hIvGWv6yNb7JAjJkn0G0ybALLNO7dbySsv+VWV7ceMb0doyXVsXYMpNx2D8X6djymJnjxWVHrdSc2G8dmwfdGmdj3GDOsacT0R4/rJiDO7cKmUZORs7ydR/FYRsRgR+BnDzL9d94QFjITVePVcgmkb5oU+MurcqRe8Zw91rqx4/0JgIUlker3FieAnlZGAtQxD8QFSXDKAyLdrzp1fVQeMuNhOUqcVYpyyObuhlCQUnyPJPEBo6IvAzgBLsdrs8J1kIYkSvtmhp5s/53UkDAADDunvPPpiOqk0NGWXJCYvlS2gk+GbSIaJ8ANMB5Jn9vMPMd/rVX0NCmWES1Zt1Ij8nEEmL/NY1oyP7VdWgZknmn3n9ylGRrJuClYfPGYaHPlmGdoW5iU8WhAaAnzb8SgDHM/NeIsoBMIOIPmLmbxJd2NhRSc7swU4/fXJWwmsrXez/KgVDsyRTCo/u0y6p85sSR/crwtH9ijI9DEFIGb4JfDbsE3vNlznmX3bEymcYJbTrouG7WX1UCmcKi78AABQRSURBVOU6FMESBKGJ4KsRl4iCRDQPwFYAU5h5tp/9NRRUjph4uVBuOL5vUm2q3O1tmov5QRAEZ3wV+Mxcy8zDAXQDMIKIYopXEtFVRFRCRCVlZWV+DidrUCadjbuNIuQry/bG5Ly/dmxU4C/600kxbYzqbS0feMPx/XD2YV1x/hHdUz1cQRAaCekqYr4LwDQAJzsce5qZi5m5uKio8dlLpy3dij/+Z6Fl3+3vL4psr962D+Me+RJ/mbLMco6eH0cvK3jnaUZJvR5tm1nOb9M8F385d7hrvntBEATfBD4RFRFRa3O7AMB4AEv96i9b+fmL3+LVb9Za9una/JY9hpb/7eqdlnPsC7oKVWdVwv0FQUgWP9XBzgBeIqIgjInlLWb+0Mf+sg6n+rE1WnBVtzYFEeE/p3RH3LaeuOgwFOQEsXG3ER1rr18rCIKQCD+9dBYAONSv9hsCp/xtesy+JZvKI9sFOUH884uVntqaMNRIqFZVE8amXRW47rg+qRmkIAhNBrEL+MjKsn0x+077x4zI9vKte8FJeqrmhgK4+aQBaJaFhUIEQchuROD7RI3HvDibTE8dncV3x3rlCIIg1BcR+D5hz3zplidnle0p4Ncn9BPtXRAEXxCB7xPlldagqoNu+x9mLN+W8LqQpOIVBMEnROD7xLRlsUFkFz+XONA4GJCPRBAEfxDp4sC5T87CkZM+93z+3DU70OvWyVi0YXdk3+79VXXq26vtXxAEIVlE4Dswp3QHNuw64Dk//UcLNwMA/jt/Y2RftzbN3E6Py446ThSCIAiJEIEfh6enr/J03tQlRr3ZF2aWRvbZc+N4RW9DEAQhlYjAj8ObJes8nVe6fT8A4NLRPSP7ahKUSXrsgiYdkyYIQgYQgR+H/ZW1WLG13LNpJ0dLeFYTR8M/ul97nDasi+OxC0b0SG6QgiAIHhGBH4fNeypwwl+m47Z/L0x8MqJlBgGgptZd4J8xvKvrsSFdW3ofoCAIQhKIwPfAG996M+3oydJ0Df+Jiw6znKeKh//p9IMBADeO6xc5Fm8yEARBqA8S0mkj7GCKaR+niLVu7qmojkbXKvfKu884OJL4TJFrpja+dHRPHNO/CK0LcvC3z5ajfWGuJfe9IAhCKhEN38a6nftj9hW1yHc9/2cvfBvZPlClCXxz4jjPrEA1747xkWMqmpaIcFD75mjdLAdXH9Mb/7pyVP0GLwiCEAcR+DYe/vTHmH1l5bEJzhTTf4xG1FZUR006qrBJjhk527pZLg7p1goAUG0LriIi3DZhEPp3bFH3gQuCICSgUQn8Oat34KvldauLW7ptHzbuOmDR0hXb9joHQ/38hTmW1wc0k87Ls9YAsFauWrDeiMT9YeOeOo1REAShPvhmMCai7gBeBtARAAN4mpn/5ld/NbVhnPvULADAivtOQSiJEoDMjLEPf5F0n/Z8ObrAd+L0YV3wwfyNMfVoBUEQ0oGfK4Q1AH7DzN8RUQsAc4loCjMv9qOzXQei2SmraxmhoPdryytrUjKGigQCf9JPhqJfh0KcfZh44giCkH58M+kw8yZm/s7cLgewBIBvkk4XtlVJJiBzMuPY2RrHjg8Ao3q3TdhOs9wQrh/XL6mnD0EQhFSRFslDRL1g1LeNyQ9MRFcRUQkRlZSV1c3+DgD7NWH7n+83oNetkzF/3S5P1zoVG7czf91uy+uVZXstr7u0Lkho0hEEQcgkvgt8IioE8C6AXzNzzGolMz/NzMXMXFxUVFTnfraVV0a2H/pkGQDgjMdnerrWyxPBlS+XWF6/992GyPacP4xDQU4woUlHEAQhk/gq8IkoB4awf42Z/+1nX3d88ENke69mky/dFltI3M6mXbHmmtxQ/Fvzj2krABg+9R1a5KMgJ+jJNCQIgpApfBP4REQAngOwhJn/4lc/iqP6tnfcf+Jfpye81q69A8AbV3kLglIBVgW5QRyoro1E3nZsmYfzirt7akMQBCEd+KnhHwngEgDHE9E882+CX531KWruuL+qNoy122OjZ3WU7f3oftFJ4+AuLdG+MC9hv8U92wAA8nOCCHPUPFRdy8gJSX1aQRCyB9/cMpl5BoC0SbzKOAuvP2zcjR7tEvu+t20ezZmTFwqi5I8n4OsV23Dhs+61aDu3LgAAFOQYfqAVVWHkhYKorgkjR7xxBEHIIhqNRJq9eofrMa/Fp8b0aRez7yCXJwfFwE5GOoSCXEPgq6eF6nA4kiRNEAQhG2g0EmnK4i2ux2rjFDDZpy3wnlvcHeMHd8RrV4yM7OvcqgDHD+wAwMikuXNflSUXzrXH9gEQ1fCVwK+oFg1fEITsoknk4n3p61Kc7lJh6qg/fx7ZJiI8c2lxzDmbdxtePJ8v3YorXi7BMDMJGhDNlZOvBH5VLaYt2woA+HrlNgADUvIeBEEQ6kujUUGP6tve1YQyd81OVx/5nfurHffrLN5khA+oGrfz1++OOUc36Txhumx+t9Zb4JcgCEI6aDQCPxAgDO4SLQ94/1lD8covRkReD7z944TeOomIZzbKN/32K6pr8W3pznr1IwiC4AeNRuBXVtdagqUuHNkDA2z55Y95aBpqva7gJklEw9eCr3KC4pYpCEL20GgE/uzVOzDH5qnTzKFc4J0fLEq67XvPHAIA6NTSvfKVfdEWAB674DC30wVBENJOoxH4iod+ekgkSlYJYZ1Xv1nreN1vxvd3bfPMQ40kn8GAVWN/U4vGzdcE/vDurQEAJw7umMTIBUEQ/KXReemco6UzCAYIeaFA3KAsxa+O7+t6TNWg7dOhEBt2HYjsL8yP3j5l0vndOwsAAIf3bGOpdiUIgpBpGpWGP+KgtjH73Dx3vvyxDEdOMlwyR/VuCyP1jzPKn/77NdbF2Dytyor9aWLuGlm4FQQhu2gUAj9sLsSO7h0bKesWdHX7fxZFtPVvVrlH6QLGk0KAYitj5WmLxPkO5iNBEIRsolEI/P3mQuleh1KFXc1cNwr1FLB2R3Iumk5VqvJyovvs9n1BEIRso1EI/EkfLQEA9HRIkPbaFSNx/1lDI6/r6iqZ4yDQ85IpnCsIgpBhGoXAV543+Q4CuEPLfFw4skfkdXUtoybJmreAi4ZvK5LSuVXUbbOoReLUyoIgCOmkUQh8hW5isaO0/zmrd+DGN+ZZjik/+3jsPhCbgsFut7/vrGg7epSvIAhCNuBnxavniWgrESUf6VRHlm0udz325W+Pw3EDjJq5kxdushzr2qbA6RJXxg/uiNJJE2P2Hz+wI2beejz+88sjMbBTS4crBUEQMoefGv6LAE72sf0Y5q2Ln6zsq+XbHPdv31vluY+rjuntmFFT0bV1QSTwShAEIZvwTeAz83QA8f0dU8xpLimQFTUueXSScbCJ464vCIKQ1WTchk9EVxFRCRGVlJWV1autRALfjW5tEpc/VFD6qjYKgiCklIwLfGZ+mpmLmbm4qKioTm3864qROPvQrmieWzc3SacIXTfE3V4QhIZKo8ilM6Zve4zp297XPlrkh1BeUWMpiSgIgtCQyLiGn07euWZ0na991lyoHSYLsoIgNFD8dMt8HcAsAAOIaD0R/cKvvrxS3KstPvn1MZHXz1xajD+dfrCna0f2bocZtxyHs8xUyYIgCA0N30w6zHyBX23Xh5qwEWV7SLdWGJ9kvvpkFncFQRCyjUZhw0+GwZ1b4obj++LCkT0zPRRBEIS00uQEPhHh/04ckOlhCIIgpJ0mtWgrCILQlBGBLwiC0EQQgS8IgtBEEIEvCILQRBCBLwiC0EQQgS8IgtBEEIEvCILQRBCBLwiC0EQgZueiIJmAiMoArKnDpe0BOJezyn4a6tgb6rgBGXsmaKjjBrJ/7D2Z2VNu+awS+HWFiEqY2b3uYBbTUMfeUMcNyNgzQUMdN9Cwx25HTDqCIAhNBBH4giAITYTGIvCfzvQA6kFDHXtDHTcgY88EDXXcQMMeu4VGYcMXBEEQEtNYNHxBEAQhASLwBUEQmgrM3GD/AJwMYBmAFQBuTXPfpQAWApgHoMTc1xbAFADLzf9ttPNvM8e5DMBJ2v7DzXZWAPg7oma2PABvmvtnA+ilXfMzs4/lAH7mYazPA9gKYJG2L6NjBXCQee4K89rcJMZ+F4AN5r2fB2BCto0dQHcA0wAsBvADgBsbyn2PM/asvu8A8gHMATDfHPefGso9T5vcylTH9R44EASwEkBvALnmhzw4jf2XAmhv2/cgzIkHwK0A/mxuDzbHl2d++CsBBM1jcwCMAkAAPgJwirn/OgBPmtvnA3hT+/KuMv+3MbfbJBjrMQAOg1VoZnSsAN4CcL65/SSAa5MY+10AbnY4N2vGDqAzgMPM7RYAfjTHl/X3Pc7Ys/q+m30Umts5MITsqIZwz9MmtzLVcb0HDowG8In2+jYAt6Wx/1LECvxlADqb250BLHMaG4BPzPF3BrBU238BgKf0c8ztEIxIP9LPMY89BeACD+PtBavQzNhYzWPbAIScPksPY78LzoIn68auXf8+gPEN6b47jL3B3HcAzQB8B2BkQ7znfv01ZBt+VwDrtNfrzX3pggFMJaK5RHSVua8jM28ytzcD6Ghuu421q7lt32+5hplrAOwG0C5OW8mSybG2A7DLPLeu7+F6IlpARM8TUZtsHjsR9QJwKAyNs0Hdd9vYgSy/70QUJKJ5MMyAU5i5wd1zP2nIAj/THMXMwwGcAuCXRHSMfpCN6ZwzMrIkaUhjNfknDFPecACbADyS2eG4Q0SFAN4F8Gtm3qMfy/b77jD2rL/vzFxr/i67ARhBRENsx7P6nvtNQxb4G2AsLim6mfvSAjNvMP9vBfAegBEAthBRZwAw/29NMNYN5rZ9v+UaIgoBaAVge5y2kiWTY90OoLV5btLvgZm3mD/sMIBnYNz7rBs7EeXAEJivMfO/zd0N4r47jb2h3HdzrLtgLDyfjAZyz9NCpmxJ9f2DYT9bBWOxRS3aHpymvpsDaKFtfw3ji/UQrItDD5rbB8O6OLQK7otDE8z9v4R1cegtc7stgNUwFobamNttPYy5F6x28IyOFcDbsC5kXZfE2Dtr2zcBeCPbxm728zKAR237s/6+xxl7Vt93AEUAWpvbBQC+AnBqQ7jn6frLuOCu1+CBCTA8CFYC+EMa++1tflGU+9cfzP3tAHwGwy1rKjRBDOAP5jiXwVzxN/cXA1hkHvsHou5f+eYXZYX55eutXXO5uX8FgJ97GO/rMB7Bq2HYEH+R6bGa93COuf9tAHlJjP0VGC5zCwB8AKsgyoqxAzgKhulgATQ3xoZw3+OMPavvO4BDAHxvjm8RgDuy4Xfp5Z6n609SKwiCIDQRGrINXxAEQUgCEfiCIAhNBBH4giAITQQR+IIgCE0EEfiCIAhNBBH4QsohIiaiR7TXNxPRXSlq+0Ui+mkq2krQzzlEtISIpmn7hhLRPPNvBxGtNrenElEXInrHx/EMJ6IJfrUvNA1E4At+UAngbCJqn+mB6GjRjl74BYArmfk4tYOZFzLzcDZC9z8A8Fvz9QnMvJGZ/ZyIhsPwhReEOiMCX/CDGhh1QG+yH7Br6ES01/w/loi+JKL3iWgVEU0ioouIaA4RLSSiPlozJxBRCRH9SESnmtcHieghIvrWTO51tdbuV0T0AYz87vbxXGC2v4iI/mzuuwNG8NFzRPSQlzdMRL2IaJG5fRkR/YeIphBRKRH9ioj+j4i+J6JviKiteV4fIvrYTMD3FRENNPefY45nPhFNJ6JcAHcDOM98ojiPiJqbCczmmO2eofX9PhF9QUTLiehOc39zIppstrmIiM7z8r6ExkUyGo8gJMPjABYQ0YNJXDMMwCAAO2CEuT/LzCOI6EYA1wP4tXleLxh5XPoAmEZEfQFcCmA3Mx9BRHkAZhLRp+b5hwEYwsyr9c6IqAuAP8ModrETwKdEdCYz301Ex8NIBVyS9Ds3GAIjy2Q+jAjLW5j5UCL6qznWR2FMitcw83IiGgngCQDHA7gDRjGODUTUmpmrzEmomJl/ZY79fgCfM/PlRNQawBwimmr2PcLsfz+Ab4loMoCeADYy80Tz+lZ1fF9CA0Y0fMEX2Miu+DKAG5K47Ftm3sTMlTBC2pXAXghDyCveYuYwMy+HMTEMBHAigEvJSI07G0Y4fT/z/Dl2YW9yBIAvmLmMjfS1r8EouJIKpjFzOTOXwUih+1/9vZCRiXIMgLfNMT8FIw87AMwE8CIRXQmj0I8TJwK41bz2CxgTSw/z2BRm3s7MBwD8G8bTykIA44noz0R0NDPvTtH7FBoQouELfvIojCIUL2j7amAqGkQUgJH4TlGpbYe112FYv6v2fCAMI8nV9cz8iX6AiMYC2Fe34deLRO8lACNP+nD7hcx8janxTwQwl4gOd2ifAPyEmZdZdhrXxdwfZv6RiA6DsQ5wLxF9xsx31+WNCQ0X0fAF32DmHTDKu/1C210Kw4QCAKfDKEWXLOcQUcC06/eGkfjqEwDXkpHWF0TUn4iaJ2hnDoBjiag9EQVhVCj6sg7jSRrzCWg1EZ0DAGQwzNzuw8yzmfkOAGUw0u6Wwyg3qPgERjESMq85VDs2nojaElEBgDNhmLe6ANjPzK/CyB55mM9vUchCROALfvMIAN1b5xkYQnY+jHJvddG+18IQ1h/BsIFXAHgWxqLsd+bi6VNI8ATLRhWkW2HkTZ8PYC4zv1+H8dSViwD8wrwXPwA4w9z/kFpIhpF6e745xsFq0RbAPTAmywVE9IP5WjEHRi77BQDeNdchhsKw888DcCeAe/1/e0K2IdkyBaERQUSXQVvcFQQd0fAFQRCaCKLhC4IgNBFEwxcEQWgiiMAXBEFoIojAFwRBaCL8fzt1IAAAAAAgyN96kAsi4QNMCB9gIqJawjHH8cW6AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c31507048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_results('./logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2,3,4,5,6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "indx = 0\n",
    "size_ratio = 0.5\n",
    "last_ind = -2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 4, 5, 6]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[-4::]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 3]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[2: -2][::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = 3\n",
    "start = stop -min(1*7, len(a)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = list(range(start,stop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-3, -2, -1, 0, 1, 2]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "5\n",
      "6\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "for idx in idxs: print(a[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 1\n",
      "sample 1\n",
      "sample 0\n",
      "sample 0\n",
      "sample 0\n",
      "sample 1\n",
      "sample 0\n",
      "sample 1\n",
      "sample 0\n",
      "sample 1\n"
     ]
    }
   ],
   "source": [
    "for i in range(10): print('sample', random.choice(list(range(2))+ list(range(0,6))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    d.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "deque([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.append(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "deque([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.append(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "deque([2, 3, 4, 5, 6, 7, 8, 9, 10, 4])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "attribute 'maxlen' of 'collections.deque' objects is not writable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-846a737cb679>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxlen\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: attribute 'maxlen' of 'collections.deque' objects is not writable"
     ]
    }
   ],
   "source": [
    "d = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "randint() takes 3 positional arguments but 5 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-26512e7ee9b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: randint() takes 3 positional arguments but 5 were given"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.randran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_atari('BreakoutNoFrameskip-v4')\n",
    "#env = wrap_atari_dqn(env)\n",
    "env = Monitor(env, 'logs', allow_early_resets=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines.deepq.dqn.DQN at 0x1c2b76f518>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DQN(CnnPolicy, env, verbose=1)\n",
    "model.learn(total_timesteps=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading a model without an environment, this model cannot be trained until it has a valid environment.\n"
     ]
    }
   ],
   "source": [
    "model = DQN.load(\"deepq_breakout_exp_lr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "i = 0\n",
    "while i<1000:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "    i+=1\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
